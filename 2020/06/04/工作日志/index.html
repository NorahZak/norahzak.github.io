<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/Kaze.png">
  <title>theme-kaze demo</title>
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/Kaze.png">
      
      <span class="navbar-logo-dsc">theme-kaze demo</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">Home </a>
    
    <a href="/archives" class="navbar-menu-item">Archive </a>
    
    <a href="/tags" class="navbar-menu-item">Tags </a>
    
    <a href="/categories" class="navbar-menu-item">Categories </a>
    
    <a href="/about" class="navbar-menu-item">About </a>
    
    <a href="/links" class="navbar-menu-item">Friends </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
  </div>
</nav>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-06-03T17:14:30.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2020-06-04</span>
    </time>
    
    
    <span class="dot"></span>
    <span>2.6k words</span>
    
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h1 id="论文工作日志"><a href="#论文工作日志" class="headerlink" title="论文工作日志"></a>论文工作日志</h1><p>[TOC]</p>
<h2 id="Motion-blurred-image-recognition"><a href="#Motion-blurred-image-recognition" class="headerlink" title="Motion-blurred image recognition"></a>Motion-blurred image recognition</h2><h3 id="Motion-blur-generation"><a href="#Motion-blur-generation" class="headerlink" title="Motion blur generation"></a>Motion blur generation</h3><h4 id="Using-consecutive-video-frames"><a href="#Using-consecutive-video-frames" class="headerlink" title="Using consecutive video frames"></a>Using consecutive video frames</h4><ul>
<li>[6] proposed a method for synthesizing motion blurred image from a pair of consecutive sharp images by training a CNN. </li>
<li>[11] also proposed to synthesize images using consecutive frames captured by a high-speed camera. </li>
</ul>
<h4 id="Using-a-single-image"><a href="#Using-a-single-image" class="headerlink" title="Using a single image"></a>Using a single image</h4><ul>
<li>Sun et al. [7] creates synthetically blurred images by convolving clean natural images with one out of 73 possible linear motion kernels. Xu et al. [8] also use linear motion kernels to create synthetically blurred images.</li>
<li>Chakrabarti [9] creates blur kernel by sampling 6 random points and fitting a spline to them. </li>
<li>[5] proposed to synthesis motion blurred images based on random trajectory generation. </li>
<li>[10]</li>
<li>[12] proposed to generate uniform blur kernels from random generated motion trajectory.</li>
</ul>
<p>There are many methods for generating motion blur kernels.  We choose the method using 2-D random motion trajectory since it can produce realistic motion blur kernels and it’s widely used by other works on motion deblurring.</p>
<h3 id="Classification-accuracy-on-ILSVRC-2012-validation-set"><a href="#Classification-accuracy-on-ILSVRC-2012-validation-set" class="headerlink" title="Classification accuracy on ILSVRC 2012 validation set"></a>Classification accuracy on ILSVRC 2012 validation set</h3><p><code>Note:</code></p>
<ul>
<li>All classification models are pretrained on <code>ImageNet</code> training set; we use implementation and pretrained parameters provided by <code>torchvision</code> for inference.</li>
<li>All object detection models are pretrained on <code>COCO2017</code> training set; we use implementation and pretrained parameters provided by <code>Detectron2</code> for inference, and use <code>COCO Python API</code> for evaluation.</li>
<li>To generate <code>realistic</code> blur images with <code>different blur degree</code>, we change the <code>PSF size</code> in range [16, 32, 48, 64], and use default <code>anxiety</code> (=0.005), <code>numT</code> (=1000) and <code>maxLength</code> = <code>PSFsize</code> for realisticity.</li>
</ul>
<p><code>Analysis:</code></p>
<ul>
<li>Apparently, models trained on clear images performs badly on motion blurred images. And the accuracy decreases as the blur gets more severe.</li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>Clear</th>
<th>p64</th>
<th>p8</th>
<th>p16</th>
<th>p24</th>
<th>p32</th>
</tr>
</thead>
<tbody><tr>
<td>vgg19_bn</td>
<td>70.18%</td>
<td>0.39660</td>
<td>0.66286</td>
<td>0.58706</td>
<td>0.515640</td>
<td>0.451240</td>
</tr>
<tr>
<td>googlenet</td>
<td>64.658%</td>
<td>0.42346</td>
<td>0.64804</td>
<td>0.587340</td>
<td>0.528820</td>
<td>0.473280</td>
</tr>
<tr>
<td>resnet101</td>
<td>74.996%</td>
<td>0.48064</td>
<td>0.73416</td>
<td>0.669020</td>
<td>0.601840</td>
<td>0.535300</td>
</tr>
<tr>
<td>densenet121</td>
<td>72.218%</td>
<td>0.49508</td>
<td>0.71518</td>
<td>0.66276</td>
<td>0.600280</td>
<td>0.547040</td>
</tr>
<tr>
<td>inception-v3</td>
<td>73.218%</td>
<td>0.53486</td>
<td>0.7086</td>
<td>0.668440</td>
<td>0.624140</td>
<td>0.583460</td>
</tr>
</tbody></table>
<h3 id="Object-detection-x6d-65-x50-x40-48-x2e-x37-x35-0-5-0-5-0-95-on-PASCAL-VOC-2012"><a href="#Object-detection-x6d-65-x50-x40-48-x2e-x37-x35-0-5-0-5-0-95-on-PASCAL-VOC-2012" class="headerlink" title="Object detection &#x6d;&#65;&#x50;&#x40;&#48;&#x2e;&#x37;&#x35;/0.5/0.5:0.95 on PASCAL VOC 2012"></a>Object detection <a href="mailto:&#x6d;&#65;&#x50;&#x40;&#48;&#x2e;&#x37;&#x35;">&#x6d;&#65;&#x50;&#x40;&#48;&#x2e;&#x37;&#x35;</a>/0.5/0.5:0.95 on PASCAL VOC 2012</h3><p><code>Note:</code></p>
<ul>
<li>All models use Resnet50 as backbone and FPN for region proposal, and are trained 3*.</li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>Clear</th>
<th>p64</th>
<th>p32</th>
<th>p16</th>
</tr>
</thead>
<tbody><tr>
<td>Mask R-CNN</td>
<td>0.419/0.546/0.445</td>
<td>0.098/0.152/0.094</td>
<td>0.132/0.194/0.122</td>
<td>0.235/0.332/0.217</td>
</tr>
<tr>
<td>Faster R-CNN</td>
<td>0.407/0.541/0.436</td>
<td>0.091/0.144/0.096</td>
<td>0.126/0.190/0.118</td>
<td>0.230/0.331/0.213</td>
</tr>
</tbody></table>
<h3 id="Test-Baidu-general-object-recognition-API"><a href="#Test-Baidu-general-object-recognition-API" class="headerlink" title="Test Baidu general object recognition API"></a>Test Baidu general object recognition API</h3><p><code>Note:</code></p>
<ul>
<li>This API can classify an image and return the top5 classes. We randomly choose 246 images (all in different classes) from Caltech256, and generate corresponding blur images with <code>PSFsize=[16, 24, 32]</code>.   </li>
<li>The evaluation is done by doing this: we use the 5 predicted classes of clear image as ground-truth, and  for a corresponding blur image, if one of its prediction is in the ground-truth, we saw it as correctly classified (i.e. top 5 accuracy). We calculate the accuracy, i.e. # of correctly classified images divided by total # of images, for each <code>PSFsize</code>. </li>
</ul>
<table>
<thead>
<tr>
<th>PSFsize</th>
<th>accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>16</td>
<td>0.793</td>
</tr>
<tr>
<td>24</td>
<td>0.717</td>
</tr>
<tr>
<td>32</td>
<td>0.671</td>
</tr>
</tbody></table>
<h3 id="Test-Amazon-image-recognition-API"><a href="#Test-Amazon-image-recognition-API" class="headerlink" title="Test Amazon image recognition API"></a>Test Amazon image recognition API</h3><p><code>Note:</code></p>
<ul>
<li>We use Amazon Rekognition API to detect objects in clear images and their blurred versions. We took the detection results on clear images as ground-truth. For each detected object in a blurred image, if there’s a ground-truth object that has the same class and an IOU&gt;=0.7, we saw it as a correct prediction. We calculate the recall and precision at different blur level.</li>
</ul>
<h3 id="Survey-on-motion-blurred-image-recognition"><a href="#Survey-on-motion-blurred-image-recognition" class="headerlink" title="Survey on motion blurred image recognition"></a>Survey on motion blurred image recognition</h3><p>The literature can be divided into 2 categories:</p>
<ul>
<li><strong>Deblurring-based</strong>: Deblur the blurred image before recognition;</li>
<li><strong>Blur invariant features based</strong>: Extract blur insensative features from motion blurred images.</li>
</ul>
<h4 id="Deblurring-based"><a href="#Deblurring-based" class="headerlink" title="Deblurring-based"></a>Deblurring-based</h4><p>There are many methods for image motion deblurring. They can thoroughly be divided into 2 categories: optimization-based method and learning-based method. We choose SRN as the representative method from the second category, and omit experiments for the first category since it has apparent disadvantage on time complexity. The classification results of Densenet121 on motion-blurred images and their deblurred (by SRN) versions are shown as below.</p>
<table>
<thead>
<tr>
<th></th>
<th>clear</th>
<th>p16</th>
<th>p16(d)</th>
<th>p24</th>
<th>p24(d)</th>
<th>p32</th>
<th>p32(d)</th>
</tr>
</thead>
<tbody><tr>
<td>acc</td>
<td>0.859</td>
<td>0.681</td>
<td>0.698</td>
<td>0.564</td>
<td>0.592</td>
<td>0.473</td>
<td>0.550</td>
</tr>
<tr>
<td>top5acc</td>
<td>0.964</td>
<td>0.868</td>
<td>0.877</td>
<td>0.785</td>
<td>0.804</td>
<td>0.710</td>
<td>0.766</td>
</tr>
</tbody></table>
<h4 id="Blur-invariant-feature-based"><a href="#Blur-invariant-feature-based" class="headerlink" title="Blur invariant feature-based"></a>Blur invariant feature-based</h4><p>Traditional methods for extracting blur-invariant features from blurred images are based on the assumption that <strong>PSF is centrally symmetric</strong>, such as LPQ and LPQ+. This assumption doesn’t hold in the scenario of motion blur, where the PSF could be in arbitrary shape. Moreover, most methods in this category are aimed at <strong>face recognition</strong> instead of <strong>general image classification</strong>.</p>
<h3 id="Classification-accuracy-on-Caltech256"><a href="#Classification-accuracy-on-Caltech256" class="headerlink" title="Classification accuracy on Caltech256"></a>Classification accuracy on Caltech256</h3><p><code>Note:</code></p>
<ul>
<li>Caltech256 is used for method verification since it’s relatively small.</li>
<li>Resnet101 performs badly because of gradient vanishing/exploding, so we choose models with fewer layers (Resnet18) or dense connections (Densenet121) .</li>
<li>Method1 - Data augmentation: Train model on mixed data (clear and blur). Observations:<ul>
<li>Comparing experiment 2-4 and 6-8, we can see that data augmentation can greatly improve model accuracy on blurred images while preserving high accuracy on clear images . This may due to the simplicity of task (general object classification) and dataset (only 256 classes). </li>
<li>Comparing experiment 11-13, we can see that increasing training data can increase the accuracy on blurred images to an upper bound of about 0.685. However, there’s still a difference of 0.175 between   accuracy on clear and blurred images, which can’t be covered by simply increasing the amount of training data.</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>No.</th>
<th>MODEL</th>
<th>train</th>
<th>test1</th>
<th>top-1/5</th>
<th>test2</th>
<th>top1/5</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>densenet121</td>
<td>clear</td>
<td>clear</td>
<td>0.859/0.964</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>densenet121</td>
<td>clear</td>
<td>p16</td>
<td>0.681/0.868</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>densenet121</td>
<td>clear</td>
<td>p24</td>
<td>0.564/0.785</td>
<td></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>densenet121</td>
<td>clear</td>
<td>p32</td>
<td>0.473/0.710</td>
<td></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>densenet121</td>
<td>clear+p16*1</td>
<td>clear</td>
<td>0.856/0.966</td>
<td></td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>densenet121</td>
<td>clear+p16*1</td>
<td>p16</td>
<td>0.779/0.924</td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>densenet121</td>
<td>clear+p16*1</td>
<td>p24</td>
<td>0.719/0.884</td>
<td></td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>densenet121</td>
<td>clear+p16*1</td>
<td>p32</td>
<td>0.665/0.848</td>
<td></td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>densenet121</td>
<td>clear+p16*1+p24*1</td>
<td>p32</td>
<td>0.676/0.856</td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>densenet121</td>
<td>clear+p16*1+p24*1+p32*1</td>
<td>p32</td>
<td>0.691/0.869</td>
<td></td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>densenet121</td>
<td>clear+p32*1</td>
<td>p32</td>
<td>0.680/0.863</td>
<td></td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>densenet121</td>
<td>clear+p32*2</td>
<td>p32</td>
<td>0.685/0.868</td>
<td>clear</td>
<td>0.836/0.956</td>
</tr>
<tr>
<td>13</td>
<td>densenet121</td>
<td>clear+p32*3</td>
<td>p32</td>
<td>0.685/0.865</td>
<td>clear</td>
<td>0.837/0.954</td>
</tr>
</tbody></table>
<h3 id="Calculate-STD-of-clear-blurred-images-and-feature-maps"><a href="#Calculate-STD-of-clear-blurred-images-and-feature-maps" class="headerlink" title="Calculate STD of clear / blurred images and feature maps"></a>Calculate STD of clear / blurred images and feature maps</h3><p><code>Note:</code></p>
<ul>
<li>We use 50 images from Caltech256 for experiment.</li>
<li>Results show that motion blur will smooth the input image as well as feature maps, both channel-wise and space-wise.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>clear</th>
<th>p16</th>
<th>p24</th>
<th>p32</th>
</tr>
</thead>
<tbody><tr>
<td>image</td>
<td>21.66</td>
<td>15.23</td>
<td>14.39</td>
<td>14.02</td>
</tr>
<tr>
<td>fmap (last layer, channel-wise)</td>
<td>0.6275</td>
<td>0.6047</td>
<td>0.5983</td>
<td>0.5834</td>
</tr>
<tr>
<td>fmap (first conv layer, space-wise)</td>
<td>1.0521e-2</td>
<td>3.9124e-3</td>
<td>3.5666e-3</td>
<td>3.3687e-3</td>
</tr>
</tbody></table>
<h3 id="Train-and-test-model-with-feature-amplification"><a href="#Train-and-test-model-with-feature-amplification" class="headerlink" title="Train and test model with feature amplification"></a>Train and test model with feature amplification</h3><p><code>Note:</code></p>
<ul>
<li>Since the feature maps of blurred images have smaller deviation than those of clear images, which may cause the model to be confused, we try to make the feature maps sharper by feature amplification.</li>
<li>We’ll try different feature amplification methods / layers / parameters.</li>
<li>The baseline model used here is DenseNet121.</li>
</ul>
<p><code>Result:</code></p>
<ul>
<li>Hardly no improvement.</li>
</ul>
<h4 id="1-Multiply-the-feature-map-by-a-constant-scalar"><a href="#1-Multiply-the-feature-map-by-a-constant-scalar" class="headerlink" title="1. Multiply the feature map by a constant scalar"></a>1. Multiply the feature map by a constant scalar</h4><table>
<thead>
<tr>
<th>factor</th>
<th>layer</th>
<th>p32</th>
<th>clear</th>
</tr>
</thead>
<tbody><tr>
<td>1.5</td>
<td>conv0.pool0</td>
<td>0.702/0.873</td>
<td>0.845/0.955</td>
</tr>
<tr>
<td>2.0</td>
<td>conv0.pool0</td>
<td>0.702/0.877</td>
<td>0.843/0.954</td>
</tr>
<tr>
<td>3.0</td>
<td>conv0.pool0</td>
<td>0.702/0.872</td>
<td>0.853/0.956</td>
</tr>
<tr>
<td>baseline</td>
<td>/</td>
<td>0.706/0.875</td>
<td>0.848/0.956</td>
</tr>
</tbody></table>
<h4 id="2-Multiply-the-feature-map-by-a-learnable-vector-in-channel-wise-manner"><a href="#2-Multiply-the-feature-map-by-a-learnable-vector-in-channel-wise-manner" class="headerlink" title="2. Multiply the feature map by a learnable vector in channel-wise manner"></a>2. Multiply the feature map by a learnable vector in channel-wise manner</h4><table>
<thead>
<tr>
<th>layer</th>
<th>n_epochs</th>
<th>lr</th>
<th>p32</th>
<th>clear</th>
</tr>
</thead>
<tbody><tr>
<td>conv0.pool0</td>
<td>15</td>
<td>1e-3</td>
<td>0.708/0.876</td>
<td>0.847/0.954</td>
</tr>
</tbody></table>
<h4 id="3-Use-instance-normalization"><a href="#3-Use-instance-normalization" class="headerlink" title="3. Use instance normalization"></a>3. Use instance normalization</h4><table>
<thead>
<tr>
<th>layer</th>
<th>n_epochs</th>
<th>affine</th>
<th>lr</th>
<th>p32</th>
<th>clear</th>
</tr>
</thead>
<tbody><tr>
<td>conv0.pool0</td>
<td>15</td>
<td>False</td>
<td>/</td>
<td>0.677/0.850</td>
<td>/</td>
</tr>
<tr>
<td>conv0.pool0</td>
<td>15</td>
<td>True</td>
<td>1e-3</td>
<td>0.675/0.852</td>
<td>/</td>
</tr>
</tbody></table>
<h3 id="Joint-motion-blurred-image-deblurring-and-recognition"><a href="#Joint-motion-blurred-image-deblurring-and-recognition" class="headerlink" title="Joint motion-blurred image deblurring and recognition"></a>Joint motion-blurred image deblurring and recognition</h3><p><code>Note:</code></p>
<ul>
<li><code>Insight:</code> <ol>
<li>Using MSEloss to train a deblurring model will decrease the distance of pixel values between deblurred image and ground-truth. However, it’s not the best loss function for describing the loss on image perceptual quality (both by human and by machine), and might lead to viewer’s unsatisfactory and bad performance of image recognition model <code>&#123;1&#125;</code>. </li>
<li>Some former works have noticed this issue and introduced perceptual loss to their method. They use a fixed pretrained feature network (e.g. vgg19) to extract perceptual features from images, which equals to using a fixed loss function. Instead, we could use learned loss function, i.e. train the feature network with deblurring model jointly.</li>
</ol>
</li>
<li><code>Baselines:</code> <ol>
<li><code>deblur-mse:</code> Deblurring model with MSEloss </li>
<li><code>deblur-per:</code> Deblurring model with MSEloss and perceptual loss </li>
<li><code>recog:</code> Recognition model pretrained on clear images and fine-tuned on deblurred images</li>
</ol>
</li>
</ul>
<h4 id="deblur-mse"><a href="#deblur-mse" class="headerlink" title="deblur-mse"></a>deblur-mse</h4><table>
<thead>
<tr>
<th>model</th>
<th>PSNR</th>
<th>SSIM</th>
</tr>
</thead>
<tbody><tr>
<td>srn-convlstm</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Training-configuration"><a href="#Training-configuration" class="headerlink" title="Training configuration"></a>Training configuration</h3><table>
<thead>
<tr>
<th>NOTE</th>
<th>MEANING</th>
<th>INFLUENCE</th>
</tr>
</thead>
<tbody><tr>
<td>cp</td>
<td>color channel permutation</td>
<td>-</td>
</tr>
<tr>
<td>cw</td>
<td>using class weights for computing loss</td>
<td>down</td>
</tr>
<tr>
<td>n</td>
<td>special normalize (mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])</td>
<td>-</td>
</tr>
<tr>
<td>cn</td>
<td>common normalize (mean=[0], std=[1])</td>
<td>-</td>
</tr>
<tr>
<td>sn</td>
<td>simple normalize (img = img * 2 - 1)</td>
<td>-</td>
</tr>
<tr>
<td>cj</td>
<td>color jittering</td>
<td>up</td>
</tr>
<tr>
<td>an(x)</td>
<td>add gaussian noise with std=x while training</td>
<td>down</td>
</tr>
<tr>
<td>tri</td>
<td>triangle lr scheduler</td>
<td>-</td>
</tr>
<tr>
<td><em>x</em>i</td>
<td>drop lr per <em>x</em> iterations</td>
<td></td>
</tr>
<tr>
<td><em>x</em>s</td>
<td>sustain lr at first <em>x</em> iterations</td>
<td></td>
</tr>
</tbody></table>
<h2 id="Deblur"><a href="#Deblur" class="headerlink" title="Deblur"></a>Deblur</h2><table>
<thead>
<tr>
<th>Model</th>
<th>Config</th>
<th>psnr</th>
<th>num-steps</th>
<th>te-time</th>
<th>tr-time</th>
</tr>
</thead>
<tbody><tr>
<td>SRN-tf-gray</td>
<td>/</td>
<td>30.26</td>
<td>523000</td>
<td>cpu-1.08s</td>
<td></td>
</tr>
<tr>
<td>SRN-tf-color</td>
<td>/</td>
<td>30.25</td>
<td>523000</td>
<td>gpu-0.38s</td>
<td></td>
</tr>
<tr>
<td>SRN-tf-gray</td>
<td>origin</td>
<td>30.255</td>
<td>523000</td>
<td>gpu-1.00s</td>
<td>4 days</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><p><strong>二分类模型评价指标</strong></p>
<p><strong><code>准确率</code></strong></p>
<p>$Accuracy=num_of_correct_samples/num_of_all_samples$</p>
<p><strong><code>精确率，召回率与F1 score</code></strong></p>
<p>$Precision=\frac{TP}{TP+FP}$</p>
<p>$Recall=\frac{TP}{TP+FN}$</p>
<p>$F1=\frac{2\times (Precision+Recall)}{Precision\times Recall}$</p>
<p><strong><code>Matthews correlation coefficient (MCC)</code></strong></p>
<p>$MCC=\frac{TP\times TN-FP\times FN}{\sqrt{(TP+FP)(TP+FN)(FP+TN)(TN+FN)}}\in[-1,1]$</p>
<p><strong>多标签分类模型</strong></p>
<p><em><strong>评价指标</strong></em></p>
<p><strong><code>mAP</code></strong></p>
<p><strong><code>Hamming loss</code></strong></p>
<p>$HammingLoss=\sum\limits_{i=1}^n\sum\limits_{j=1}^mxor(y_{ij},\hat y),n=num_samples,m=num_classes$</p>
<p><em><strong>模型及损失函数</strong></em></p>
<p><strong><code>HCP(Hypothesis CNN Pooling)</code></strong></p>
<p>Wei, Yunchao, et al. “HCP: A flexible CNN framework for multi-label image classification.” <em>IEEE transactions on pattern analysis and machine intelligence</em> 38.9 (2015): 1901-1907.</p>
<p><strong>Method</strong></p>
<ul>
<li><strong><code>Region proposal</code></strong> Using an off-the-shelf region proposal method to extract region proposals from an multi-class image that has several objects of different categories.</li>
<li><strong><code>Proposal selection</code></strong> Reduce the number of proposals that are extracted in above step.</li>
<li><strong><code>Classification</code></strong> Using traditional single-label classification models (VGG, GoogleNet, etc.) to classify each proposed region that is preserved in above step.</li>
<li><strong><code>Max pooling</code></strong> Using max pooling to aggregate all the predictions into one multi-label prediction.</li>
</ul>
<p><strong>Shortage</strong></p>
<p>This model is complicated and not end-to-end.</p>
<ul>
<li><strong><code>Motion Deblurring of Faces</code></strong></li>
</ul>
<p><em>Chrysos, Grigorios G., Paolo Favaro, and Stefanos Zafeiriou. “Motion deblurring of faces.” International journal of computer vision 127.6-7 (2019): 801-823.</em></p>
<ul>
<li><strong><code>DeepDeblur: fast one-step blurry face images restoration</code></strong> <a href="../MotionDeblur/deepdeblur_2017_cvpr.pdf">paper</a></li>
</ul>
<p><em>Wang, Lingxiao, Yali Li, and Shengjin Wang. “DeepDeblur: fast one-step blurry face images restoration.” arXiv preprint arXiv:1711.09515 (2017).</em></p>
<p>This paper uses CNN for one-step motion blurred face image restoration.</p>
<ol>
<li>Model: 6 stacked Inception modules with residual connections</li>
<li>Training data: The face images are from CASIA WebFace Database. The corresponding blurred images are generated by convolving sharp images with synthesized blur kernels using method in [4].</li>
<li>Loss function: $L_{total}=L_{data}+\alpha L_{TV}+\beta L_{facial}.$ The total loss consists of 3 terms: pixel-wise L2 loss, TV loss (for smoothness and avoiding artifacts) and facial structure loss (for preserving facial structure and improve facial recognition result on deblurred face images).</li>
<li>Evaluation: The model is evaluated qualitatively in 3 ways - inference time, PSNR and facial identification accuracy. The test images are a small subset of LFW and FaceScrub. The corresponding blurred images are generated by convolving sharp images with 8 benchmark blur kernels from [3]. The facial identification accuracy is computed using face verification model trained on MS-Celeb-1M.</li>
</ol>
<ul>
<li><strong><code>Deep Semantic Face Deblurring</code></strong> <a href="../MotionDeblur/Shen_Deep_Semantic_Face_CVPR_2018_paper.pdf">paper</a> <a target="_blank" rel="noopener" href="https://sites.google.com/site/ziyishenmi/cvpr18_face_deblur">project</a></li>
</ul>
<p><em>Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 8260-8269</em></p>
<p><strong>This paper proposed to deblur face images using semantic information from external data as image prior.</strong> </p>
<p>Since motion deblur is an ill-posed problem, it requires accurate image priors. Different from general natural images, face images are highly structural and have many common components (e.g. eyes, noses, etc.), which can serve as a strong image prior, and help better recovering sharp images. Thus, this paper exploits face image prior by using the parsing result of face images to supervise the deblurring process.</p>
<p>Specifically, it uses multi-scale encoder-decoder network for deblurring, with mainly 2 modifications:</p>
<ul>
<li>Concatenate the parsing mask with blurred image as input;</li>
<li>Using $L_1$ loss + local structural loss + perceptual loss + adverserial loss.</li>
</ul>
<p>The parsing mask serves as a global prior for deblurring, and the local structural loss (the sum of $L_1$ loss of each face component) help the network better recover the face components (eyes, noses, etc.) since they are relatively smaller but more important than other parts of face image (background, hair, skin, etc.).</p>
<ul>
<li><strong><code>Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks</code></strong> <a href="../MotionDeblur/1907.13106v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/rajeevyasarla/UMSN-Face-Deblurring">project</a></li>
</ul>
<p><em>Yasarla, Rajeev, Federico Perazzi, and Vishal M. Patel. “Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks.” arXiv preprint arXiv:1907.13106 (2019).</em></p>
<p><strong>This paper also exploits the semantic information for face deblurring</strong> . </p>
<p>It first <strong>uses different branches (Multi-Stream Network) to deblur different semantic parts in face image</strong> (i.e. hair, skin, background, facial features), then combines these results to get the final result. And they designed a <strong>Confidence Network to predict the confidence of the deblurring result for each class</strong> , and uses them as the loss weight for each class in training phase .</p>
<ul>
<li><strong><code>Human-aware motion deblurring</code></strong></li>
</ul>
<p><em>Shen, Ziyi, et al. “Human-Aware Motion Deblurring.” Proceedings of the IEEE International Conference on Computer Vision. 2019.</em></p>
<p>It argues that using end-to-end CNN for dynamic motion deblurring requires the network to learn semantic level features to recover the details. Since foreground objects often have different semantic features from background, it might be ambiguous to learn all features with one decoder. </p>
<p>Thus, <strong>this paper proposed to use seperate decoders for recovering objects (specifically, this paper concentrates on human) and background.</strong> The $L_2$ loss of human regions is backpropagated to human decoder only, and loss of background is backpropagated to background decoder only, so these encoders can concentrate on learning domain-specific semantic features.</p>
<ul>
<li><strong><code>A Variational EM Framework With Adaptive Edge Selection for Blind Motion Deblurring</code></strong></li>
</ul>
<p><em>Liuge Yang, Hui Ji; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 10167-10176</em></p>

  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/about">theme-kaze</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/2020/06/04/%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97/">http://example.com/2020/06/04/%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2020/08/03/面试记录/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2020/05/17/TED-note/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97"><span class="toc-text">论文工作日志</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motion-blurred-image-recognition"><span class="toc-text">Motion-blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-blur-generation"><span class="toc-text">Motion blur generation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-consecutive-video-frames"><span class="toc-text">Using consecutive video frames</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-a-single-image"><span class="toc-text">Using a single image</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-ILSVRC-2012-validation-set"><span class="toc-text">Classification accuracy on ILSVRC 2012 validation set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-detection-x6d-65-x50-x40-48-x2e-x37-x35-0-5-0-5-0-95-on-PASCAL-VOC-2012"><span class="toc-text">Object detection mAP@0.75&#x2F;0.5&#x2F;0.5:0.95 on PASCAL VOC 2012</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Baidu-general-object-recognition-API"><span class="toc-text">Test Baidu general object recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Amazon-image-recognition-API"><span class="toc-text">Test Amazon image recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Survey-on-motion-blurred-image-recognition"><span class="toc-text">Survey on motion blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Deblurring-based"><span class="toc-text">Deblurring-based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Blur-invariant-feature-based"><span class="toc-text">Blur invariant feature-based</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-Caltech256"><span class="toc-text">Classification accuracy on Caltech256</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Calculate-STD-of-clear-blurred-images-and-feature-maps"><span class="toc-text">Calculate STD of clear &#x2F; blurred images and feature maps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-and-test-model-with-feature-amplification"><span class="toc-text">Train and test model with feature amplification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Multiply-the-feature-map-by-a-constant-scalar"><span class="toc-text">1. Multiply the feature map by a constant scalar</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Multiply-the-feature-map-by-a-learnable-vector-in-channel-wise-manner"><span class="toc-text">2. Multiply the feature map by a learnable vector in channel-wise manner</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Use-instance-normalization"><span class="toc-text">3. Use instance normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Joint-motion-blurred-image-deblurring-and-recognition"><span class="toc-text">Joint motion-blurred image deblurring and recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#deblur-mse"><span class="toc-text">deblur-mse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-configuration"><span class="toc-text">Training configuration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deblur"><span class="toc-text">Deblur</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-works"><span class="toc-text">Related works</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/Kaze.png" class="author-img">

<p class="author-name">theme-kaze</p>
<p class="author-description">designed by theme-kaze</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>30</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>0</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>0</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97"><span class="toc-text">论文工作日志</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motion-blurred-image-recognition"><span class="toc-text">Motion-blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-blur-generation"><span class="toc-text">Motion blur generation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-consecutive-video-frames"><span class="toc-text">Using consecutive video frames</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-a-single-image"><span class="toc-text">Using a single image</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-ILSVRC-2012-validation-set"><span class="toc-text">Classification accuracy on ILSVRC 2012 validation set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-detection-x6d-65-x50-x40-48-x2e-x37-x35-0-5-0-5-0-95-on-PASCAL-VOC-2012"><span class="toc-text">Object detection mAP@0.75&#x2F;0.5&#x2F;0.5:0.95 on PASCAL VOC 2012</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Baidu-general-object-recognition-API"><span class="toc-text">Test Baidu general object recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Amazon-image-recognition-API"><span class="toc-text">Test Amazon image recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Survey-on-motion-blurred-image-recognition"><span class="toc-text">Survey on motion blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Deblurring-based"><span class="toc-text">Deblurring-based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Blur-invariant-feature-based"><span class="toc-text">Blur invariant feature-based</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-Caltech256"><span class="toc-text">Classification accuracy on Caltech256</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Calculate-STD-of-clear-blurred-images-and-feature-maps"><span class="toc-text">Calculate STD of clear &#x2F; blurred images and feature maps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-and-test-model-with-feature-amplification"><span class="toc-text">Train and test model with feature amplification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Multiply-the-feature-map-by-a-constant-scalar"><span class="toc-text">1. Multiply the feature map by a constant scalar</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Multiply-the-feature-map-by-a-learnable-vector-in-channel-wise-manner"><span class="toc-text">2. Multiply the feature map by a learnable vector in channel-wise manner</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Use-instance-normalization"><span class="toc-text">3. Use instance normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Joint-motion-blurred-image-deblurring-and-recognition"><span class="toc-text">Joint motion-blurred image deblurring and recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#deblur-mse"><span class="toc-text">deblur-mse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-configuration"><span class="toc-text">Training configuration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deblur"><span class="toc-text">Deblur</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-works"><span class="toc-text">Related works</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
  </div>
</div>
  </article>
  
  <article class="card card-content">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97"><span class="toc-text">论文工作日志</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motion-blurred-image-recognition"><span class="toc-text">Motion-blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-blur-generation"><span class="toc-text">Motion blur generation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-consecutive-video-frames"><span class="toc-text">Using consecutive video frames</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using-a-single-image"><span class="toc-text">Using a single image</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-ILSVRC-2012-validation-set"><span class="toc-text">Classification accuracy on ILSVRC 2012 validation set</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-detection-x6d-65-x50-x40-48-x2e-x37-x35-0-5-0-5-0-95-on-PASCAL-VOC-2012"><span class="toc-text">Object detection mAP@0.75&#x2F;0.5&#x2F;0.5:0.95 on PASCAL VOC 2012</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Baidu-general-object-recognition-API"><span class="toc-text">Test Baidu general object recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Test-Amazon-image-recognition-API"><span class="toc-text">Test Amazon image recognition API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Survey-on-motion-blurred-image-recognition"><span class="toc-text">Survey on motion blurred image recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Deblurring-based"><span class="toc-text">Deblurring-based</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Blur-invariant-feature-based"><span class="toc-text">Blur invariant feature-based</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-accuracy-on-Caltech256"><span class="toc-text">Classification accuracy on Caltech256</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Calculate-STD-of-clear-blurred-images-and-feature-maps"><span class="toc-text">Calculate STD of clear &#x2F; blurred images and feature maps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-and-test-model-with-feature-amplification"><span class="toc-text">Train and test model with feature amplification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Multiply-the-feature-map-by-a-constant-scalar"><span class="toc-text">1. Multiply the feature map by a constant scalar</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Multiply-the-feature-map-by-a-learnable-vector-in-channel-wise-manner"><span class="toc-text">2. Multiply the feature map by a learnable vector in channel-wise manner</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Use-instance-normalization"><span class="toc-text">3. Use instance normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Joint-motion-blurred-image-deblurring-and-recognition"><span class="toc-text">Joint motion-blurred image deblurring and recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#deblur-mse"><span class="toc-text">deblur-mse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-configuration"><span class="toc-text">Training configuration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deblur"><span class="toc-text">Deblur</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-works"><span class="toc-text">Related works</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/Python 闭包的作用/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/C++ const关键字的用法/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-28</div>
        <a href="/2020/08/28/面试常见问题/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-07</div>
        <a href="/2020/08/07/集成学习/"><div class="recent-posts-item-content"></div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2022
        </span>
        <a href="/" class="footer-link">theme-kaze demo </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('style', 'width: 100%; display: flex; justify-content: center;');
      img[i].parentElement.insertBefore(wrapper, img[i]);
      wrapper.appendChild(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>