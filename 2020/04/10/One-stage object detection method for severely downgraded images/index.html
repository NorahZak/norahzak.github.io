<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/Kaze.png">
  <title>theme-kaze demo</title>
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/Kaze.png">
      
      <span class="navbar-logo-dsc">theme-kaze demo</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">Home </a>
    
    <a href="/archives" class="navbar-menu-item">Archive </a>
    
    <a href="/tags" class="navbar-menu-item">Tags </a>
    
    <a href="/categories" class="navbar-menu-item">Categories </a>
    
    <a href="/about" class="navbar-menu-item">About </a>
    
    <a href="/links" class="navbar-menu-item">Friends </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
  </div>
</nav>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-04-10T01:21:14.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2020-04-10</span>
    </time>
    
    
    <span class="dot"></span>
    <span>2.7k words</span>
    
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h1 id="One-stage-classification-method-for-motion-blurred-images"><a href="#One-stage-classification-method-for-motion-blurred-images" class="headerlink" title="One-stage classification method for motion blurred images"></a>One-stage classification method for motion blurred images</h1><p>[TOC]</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Image classification has many practical applications in real world and is well researched. However, today’s most methods are aimed at processing images of good quality, while in many real cases the images can be severely downgraded by some facts like blur, noise and fog. </p>
<p>A natural thought is to firstly use image recovery methods to recover high quality images from low quality counterparts. However, this introduced extra time and space cost. If we only need to perform vision tasks(classification, detection, etc), then there’s no need to spend effort on recovering the sharp image. Instead, we can directly extract features from downgraded images and perform vision task on feature maps. In this paper, we focus on one downgrading factor - motion blur. To let the model learn how to extract features in motion blurred images from data, we propose a method for generating motion blurred image from single sharp image, and use it to extend exist classification dataset. We also design a novel classification model aimed at improving robustness and accuracy on motion blurred images. Experiments showed that our proposed model can perform better than traditional classification methods on motion blurred images.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>Image classification is a classical problem in computer vision area, which is well researched and has many practical applications in real world.</p>
<p>(Survey on image classification, keep it simple)</p>
<p>Though these methods can get promising results on images with good quality, they can suffer a lot from the downgradation of input images, such as low resolution, fog, noise, blur, etc. Among these factors, motion blur is one of the most common and harmful factors. Fig 1 showed how motion blur will affect the result of object detection.</p>
<p><img src="/Users/apple/Documents/Note/pic.jpg" alt="pic" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/Users/apple/Documents/Note/pic.jpg" class="lozad post-image"></p>
<p>To tackle with this, the direct solution is to firstly use motion deblurring methods to recover high quality images from blurred counterparts, then perform classification on recovered images. However, this method needs to recover the sharp image as an intermediate step. If our aim is just performing classification and use the result to let the machine make automatic decisions (without letting human seeing the recovered image), then we can omit this intermediate step of recovering the visual pleasant sharp image which introduced extra time and space cost, and directly perform classification based on feature maps extracted from blurred image. </p>
<p>There has been some research efforts on directly extracting blur-invariable features from blurred images. [1, 2] proposed some methods based on the assumption that the PSF is symmetric, which is often violated in practical situations. In contrast, we make the model learn how to extract features from blurred image from data, without make any extra assumptions about the image.</p>
<p>We proposed a novel image classification model designed for motion blurred images. Due to lack of dataset with motion blurred images and corresponding class labels, we choose to synthesize blur images from sharp ones in traditional image classification dataset, then train and evaluate our model on this synthesized dataset. Comparing with traditional classification methods, our method use different way for extracting features to gain robustness on motion blurred images. We first use a blur segmentation module to seperate blurred regions from sharp regions, then use two different networks to extract features form these 2 kinds of regions seperately. </p>
<p>Our contributions are 3 folds:</p>
<ol>
<li>We propose a new motion blurred image generating method using single sharp image, which is more realistic than previous methods.</li>
<li>We propose a novel classification model, which has more robustness on motion blurred images than traditional methods.</li>
<li>As a side product, our motion blur segmentation module is proven to be simple and effective for the task of motion blur segmentation, which has many applications such as motion analysis and image restoration. </li>
</ol>
<h2 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2.Related works"></a>2.Related works</h2><h3 id="1-Blur-segmentation"><a href="#1-Blur-segmentation" class="headerlink" title="1.Blur segmentation"></a>1.Blur segmentation</h3><p>[14] gaussian filter</p>
<p>[15-17] motion blur</p>
<p>[18-19] learning based methods with hand-designed features</p>
<p>[5] learning based methods with hand-designed and learned features</p>
<p>[4] deep learning methods with no assumption on images and no hand-designed features</p>
<p><u>Key approaches which address only motion blur detection are: [15] proposed 1-D box filters for spatially varying motion blur segmentation.[16] proposed alternating between two tasks of blur kernel estimation and blur segmentation. However, their method is restricted to motion blur. The approach in [17] handles both types of blur by defining a singular value distribution feature and alpha-channel extraction step.</u> </p>
<p><u>Learning based approaches like [18] and [19] propose to use several hand-designed local blur features dependent on local patch properties like power spectrum, histogram and gradient profiles and feed them to a classifier.</u> </p>
<p><u>However, accurate detection under varying degrees of blur and accurate classification of homogeneous regions is still a challenge for these approaches. [5] proposed a shallow CNN to obtain deep feature representations of an image patch. They found that such patch level representations are useful but not sufficient in themselves for robust blur detection and hence they concatenate handcrafted features along with these representations for the task. Moreover, their method does not address motion blur.</u> [4] uses end-to-end network and can deal with motion blur, but it requires patch level classification and post processing based on MRF cost, so it’s time consuming and can not satisfy the need of real-time processing. </p>
<p>Instead of the methods mentioned above, we propose a novel end-to-end motion blur segmentation network with more accuracy and less time cost, and train it on our synthesized dataset.</p>
<h3 id="2-Motion-deblurring-and-blur-image-synthesis"><a href="#2-Motion-deblurring-and-blur-image-synthesis" class="headerlink" title="2.Motion deblurring and blur image synthesis"></a>2.Motion deblurring and blur image synthesis</h3><p>Motion deblur is a well-researched problem that has many applications in real world. Traditional methods tried to use optimization to estimate the latent sharp image from its motion blurred counterpart, and use hand-crafted image priors to regularize the optimization process. However, these methods are very time-comsuming since they have to estimate the sharp image and blur kernel in an iterative way to make the estimation more accurate. And to simplify the problem, they made some assumption such as uniform blur, linear blur and natural image prior, which are often violated in real world.</p>
<p>Due to these holdbacks and the recent success of deep learning in image processing, many dl-based methods appeared in this area. They made no assumptions on the blur kernel and natural image priors, instead they let the model to learn these features automatically from data in an end-to-end way. This requires large datasets consist of pairs of motion blurred and corresponding sharp image.  However, it’s hard to obtain such image pairs in real world. Therefore, many researchers have proposed to generated blur images from sharp images.</p>
<p>[6] proposed a method for synthesizing motion blurred image from a pair of consecutive sharp images by training a CNN. [11] also proposed to synthesize images using consecutive frames captured by a high-speed camera. <u>Sun et al. [7] creates synthetically blurred images by convolving clean natural images with one out of 73 possible linear motion kernels. Xu et al. [8] also use linear motion kernels to create synthetically blurred images. Chakrabarti [9] creates blur kernel by sampling 6 random points and fitting a spline to them.</u> [10] proposed to synthesis motion blurred images based on random trajectory generation. [12] proposed to generate uniform blur kernels from random generated motion trajectory.</p>
<p>In conclusion, there are mainly 2 kinds of motion blur image generation methods:</p>
<p>1.Generate blur image from consecutive video frames [6, 11]. However, [6] has very limited diversity of scenes, since it is captured by human with a high-speed camera in city situation; [11] can provide more diverse scenes, but it doesn’t have any extra information to be utilized (class, bounding box, etc), thus it’s hard to use it to exploit classification or detection model specially designed for motion blur images.</p>
<p>2.Generate blur image from a single sharp frame. There are many synthesizing methods that belong to this category[7, 8, 9, 10]. But they all failed to consider about the outline of foreground objects, so they can’t mimic the motion of foreground objects and thus the result are not so realistic.</p>
<p>We generate blur images using images in ?, which has both instance segmentation masks and classification labels. It provides outline of the foreground objects so we can use it to better mimic the motion of foreground objects. </p>
<p>Since this dataset has more diversity, models trained on it may have better generalize ability than models trained on other dataset like GOPRO dataset. And since it has information like class label and object bounding box, it can be used to further exploit the features of motion blur images. </p>
<h3 id="3-Image-classification"><a href="#3-Image-classification" class="headerlink" title="3.Image classification"></a>3.Image classification</h3><p>Image classification is a classical and basic problem in computer vision area. As the computing ability and storage of computers increasing and the emerging of large labeled dataset, nowadays data-motivated deep learning methods have achieved state-of-the-art performance. Inside this catagory, there are many classical models like ResNet, VGGNet, GoogleNet. In this paper, we choose VGGNet as our baseline due to its simplicity and efficiency.</p>
<h2 id="3-Dataset-generation"><a href="#3-Dataset-generation" class="headerlink" title="3.Dataset generation"></a>3.Dataset generation</h2><p>For generating motion blur images, we assume the motion is linear, which is often valid in real world. Since the motion blur is caused by 2 reasons: camera shake and object motion, we have to mimic both kinds of blur.</p>
<p>To mimic blur caused by object motion, we first get the instance segmentation masks of an image(human-annotated or generated by off-the-shelf instance segmentation algorithms). Then we randomly choose some objects in image, generate blur kernels in random angle and radius, then blur the object with convolution. </p>
<p>To mimic camera shake, we give the background and all objects that are not blurred in the above stage the same blur kernel, and randomly blur it as well. </p>
<p>The whole process can be expressed as follows:</p>
<pre class="highlight"><span class="line">Generate motion blurred image(image):</span><br><span class="line">	blurred_region = None</span><br><span class="line">	result = None</span><br><span class="line">	# generate moving object</span><br><span class="line">	for mask in object_masks:</span><br><span class="line">			get blur prob p in range(0, 1.0)</span><br><span class="line">			if p &gt; 0.5:</span><br><span class="line">					get random moving distance d in range(0, 15)</span><br><span class="line">					get random moving angle a in range(0, 180)</span><br><span class="line">					kernel = GetBlurKernel(d, a)</span><br><span class="line">					convolved_img = image * kernel</span><br><span class="line">					blurred_object = convolved_img &amp; mask</span><br><span class="line">					result += blurred_object</span><br><span class="line">          blurred_region += mask</span><br><span class="line">	# mimic blur caused by camera shake</span><br><span class="line">	get blur prob p in range(0, 1.0)</span><br><span class="line">	if p &gt; 0.5:</span><br><span class="line">			sharp_region = 1-blurred_region</span><br><span class="line">			get random moving distance d in range(0, 15)</span><br><span class="line">			get random moving angle a in range(0, 180)</span><br><span class="line">			kernel = GetBlurKernel(d, a)</span><br><span class="line">			convolved_img = image * kernel</span><br><span class="line">			blurred_background = convolved_img &amp; sharp_region</span><br><span class="line">			result += blurred_background</span><br><span class="line">      blurred_region += sharp_region</span><br><span class="line">	return blurred_region, result</span><br></pre>



<h2 id="4-Methods"><a href="#4-Methods" class="headerlink" title="4.Methods"></a>4.Methods</h2><p>To gain robustness on motion blurred images, we make two modifications on VGG model. The first several layers in VGG19 are used to extract low-level vision features. However, while it’s easy to extract these features from sharp regions, it can be hard to accurately extract features from motion blurred regions. So we propose to firstly seperate sharp regions from blurred regions using motion blur segmentation module, than extract features from these two kind of regions using different sub-network.</p>
<h3 id="1-Motion-blur-segmentation-module"><a href="#1-Motion-blur-segmentation-module" class="headerlink" title="1. Motion blur segmentation module"></a>1. Motion blur segmentation module</h3><p>Since motion blur segmentation aims at classifying each pixel as motion blurred or sharp, it shares some common characteristics with the task of sementic segmentation. Therefore, we adopt U-net structure with skip connections, which can combine high-level and low-level features effectively and has been proven to be efficient on the task of sementic segmentation. </p>
<p>However, unlike sementic segmentation, motion blur segmentation only requires low-level features, and doesn’t need high-level sementic features. Thus we make our network more shallower than those used in sementic segmentation. We train our network on the synthesized dataset described in section 3, since we have the ground-truth motion blur map of the synthesized blurred images.</p>
<h3 id="2-Low-level-feature-extract-sub-networks"><a href="#2-Low-level-feature-extract-sub-networks" class="headerlink" title="2. Low-level feature extract sub-networks"></a>2. Low-level feature extract sub-networks</h3><p>We design two different low-level feature extraction sub-networks for sharp and blurred regions seperately, since these two kinds of regions have very different characteristics, such as color and texture. </p>
<p>For sharp region feature extraction, we simply follow the methods in VGG19, i.e. using several convolution layers. For motion blurred region feature extraction, since it doesn’t have clear textures, it takes more effort to distinguish its low-level vision cues. Also, since the image signal is averaged among the blurred region, it needs wider receptive field to recover local features. Thus we use a deeper sub-network to extract features from blurred regions.</p>
<h2 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5.Experiment"></a>5.Experiment</h2><p>We choose VGG19 [13] as our classification model baseline, since it’s simple and effective. We develop our project using PyTorch. </p>
<p>First, we trained vgg19 model on 1151 clear images with 20 different classes from PASCAL VOC train split, which has corresponding segmentation masks. We followed training configuration described in [13]. For data augmentation, we used random horizental flip, color channels shuffle, rescaling and random crop. For optimization, we choose mini-batch gradient descent with batch size of 64, momentum of 0.9, initial learning rate of 0.01 and weight penalty of 0.0005. We trained the model for 120 epochs, and decreased learning rate by a factor of 10 in every 30 epochs.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. Flusser and T. Suk, “Degraded image analysis: an invariant approach,” in <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 20, no. 6, pp. 590-603, June 1998.</p>
<p>[2] J. Flusser, T. Suk, and S. Saic, “Recognition of Images Degraded<br>by Linear Motion Blur Without Restoration,” Computing Suppl.,<br>vol. 11, pp. 37-51, 1996</p>
<p>[3] Miyamoto R., Kobayashi S. (2016) Object Detection Based on Image Blur Using Spatial-Domain Filtering with Haar-Like Features. In: Bebis G. et al. (eds) Advances in Visual Computing. ISVC 2016. Lecture Notes in Computer Science, vol 10072. Springer, Cham</p>
<p>[4] Su, Bolan &amp; Lu, Shijian &amp; Tan, Chew Lim. (2011). Blurred Image Region Detection and Classification. 1397-1400. 10.1145/2072298.2072024. </p>
<p>[5] Jianping Shi, Li Xu, Jiaya Jia, “Discriminative Blur Detection Features”, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 2965-2972</p>
<p>[6] Brooks, Tim, and Jonathan T. Barron. “Learning to Synthesize Motion Blur.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>
<p>[7] J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal. 2015. 3, 5, 7, 8</p>
<p>[8] L. Xu, J. S. J. Ren, C. Liu, and J. Jia. Deep convolutional neural network for image deconvolution. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1, NIPS’14, pages 1790–1798, Cambridge, MA, USA, 2014. MIT Press. 5</p>
<p>[9] A. Chakrabarti. A neural approach to blind motion deblurring. In Lecture Notes in Computer Science (including sub-series Lecture Notes in Artificial Intelligence and LectureNotes in Bioinformatics), 2016. 3, 5</p>
<p>[10] Kupyn, Orest, et al. “Deblurgan: Blind motion deblurring using conditional adversarial networks.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2018.</p>
<p>[11] Nah, Seungjun, Tae Hyun Kim, and Kyoung Mu Lee. “Deep multi-scale convolutional neural network for dynamic scene deblurring.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2017.</p>
<p>[12] Schmidt, Uwe, et al. “Discriminative non-blind deblurring.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2013.</p>
<p>[13] Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” <em>arXiv preprint arXiv:1409.1556</em> (2014).</p>
<p>[14] Elder, James H., and Steven W. Zucker. “Local scale control for edge detection and blur estimation.” <em>IEEE Transactions on pattern analysis and machine intelligence</em> 20.7 (1998): 699-716.</p>
<p>[15]  AyanChakrabarti,ToddZickler,andWilliamTFreeman,“Analyzing spatially-varying blur,” in <em>Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</em>. IEEE, 2010, pp. 2512–2519. </p>
<p>[16]  Te-LiWang,Kuan-YunLee,andYu-ChiangFrankWang,“Partial image blur detection and segmentation from a single snap- shot,” in <em>Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on</em>. IEEE, 2017, pp. 1907–1911. </p>
<p>[17]  Bolan Su, Shijian Lu, and Chew Lim Tan, “Blurred image region detection and classification,” in <em>Proceedings of the 19th ACM international conference on Multimedia</em>. ACM, 2011, pp. 1397–1400. </p>
<p>[18]  Renting Liu, Zhaorong Li, and Jiaya Jia, “Image partial blur detection and classification,” in <em>Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on</em>. IEEE, 2008, pp. 1–8. </p>
<p>[19]  Jianping Shi, Li Xu, and Jiaya Jia, “Discriminative blur detection features,” in <em>Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition</em>, 2014, pp. 2965–2972. </p>

  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/about">theme-kaze</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/2020/04/10/One-stage%20object%20detection%20method%20for%20severely%20downgraded%20images/">http://example.com/2020/04/10/One-stage%20object%20detection%20method%20for%20severely%20downgraded%20images/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2020/04/10/ObjectDetection/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2020/04/10/Paper draft/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#One-stage-classification-method-for-motion-blurred-images"><span class="toc-text">One-stage classification method for motion blurred images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1.Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-works"><span class="toc-text">2.Related works</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Blur-segmentation"><span class="toc-text">1.Blur segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Motion-deblurring-and-blur-image-synthesis"><span class="toc-text">2.Motion deblurring and blur image synthesis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Image-classification"><span class="toc-text">3.Image classification</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Dataset-generation"><span class="toc-text">3.Dataset generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Methods"><span class="toc-text">4.Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Motion-blur-segmentation-module"><span class="toc-text">1. Motion blur segmentation module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Low-level-feature-extract-sub-networks"><span class="toc-text">2. Low-level feature extract sub-networks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiment"><span class="toc-text">5.Experiment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-text">References</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/Kaze.png" class="author-img">

<p class="author-name">theme-kaze</p>
<p class="author-description">designed by theme-kaze</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>30</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>0</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>0</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#One-stage-classification-method-for-motion-blurred-images"><span class="toc-text">One-stage classification method for motion blurred images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1.Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-works"><span class="toc-text">2.Related works</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Blur-segmentation"><span class="toc-text">1.Blur segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Motion-deblurring-and-blur-image-synthesis"><span class="toc-text">2.Motion deblurring and blur image synthesis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Image-classification"><span class="toc-text">3.Image classification</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Dataset-generation"><span class="toc-text">3.Dataset generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Methods"><span class="toc-text">4.Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Motion-blur-segmentation-module"><span class="toc-text">1. Motion blur segmentation module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Low-level-feature-extract-sub-networks"><span class="toc-text">2. Low-level feature extract sub-networks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiment"><span class="toc-text">5.Experiment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-text">References</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
  </div>
</div>
  </article>
  
  <article class="card card-content">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#One-stage-classification-method-for-motion-blurred-images"><span class="toc-text">One-stage classification method for motion blurred images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1.Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-works"><span class="toc-text">2.Related works</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Blur-segmentation"><span class="toc-text">1.Blur segmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Motion-deblurring-and-blur-image-synthesis"><span class="toc-text">2.Motion deblurring and blur image synthesis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Image-classification"><span class="toc-text">3.Image classification</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Dataset-generation"><span class="toc-text">3.Dataset generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Methods"><span class="toc-text">4.Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Motion-blur-segmentation-module"><span class="toc-text">1. Motion blur segmentation module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Low-level-feature-extract-sub-networks"><span class="toc-text">2. Low-level feature extract sub-networks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiment"><span class="toc-text">5.Experiment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-text">References</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/Python 闭包的作用/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/C++ const关键字的用法/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-28</div>
        <a href="/2020/08/28/面试常见问题/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-07</div>
        <a href="/2020/08/07/集成学习/"><div class="recent-posts-item-content"></div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2022
        </span>
        <a href="/" class="footer-link">theme-kaze demo </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('style', 'width: 100%; display: flex; justify-content: center;');
      img[i].parentElement.insertBefore(wrapper, img[i]);
      wrapper.appendChild(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>