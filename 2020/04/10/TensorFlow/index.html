<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/Kaze.png">
  <title>theme-kaze demo</title>
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/Kaze.png">
      
      <span class="navbar-logo-dsc">theme-kaze demo</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">Home </a>
    
    <a href="/archives" class="navbar-menu-item">Archive </a>
    
    <a href="/tags" class="navbar-menu-item">Tags </a>
    
    <a href="/categories" class="navbar-menu-item">Categories </a>
    
    <a href="/about" class="navbar-menu-item">About </a>
    
    <a href="/links" class="navbar-menu-item">Friends </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
  </div>
</nav>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2020-04-10T01:21:14.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2020-04-10</span>
    </time>
    
    
    <span class="dot"></span>
    <span>3.5k words</span>
    
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><p>[TOC]</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>TensorFlow使用静态计算图，在执行计算图之前，必须定义好所有数据节点和所有操作节点。其中，外部数据节点（运行时才确定）使用<code>tf.placeholder()</code>方法来定义，内部数据节点（运行前就已经定义好）使用 <code>tf.Variable()</code>方法来定义，而操作节点则使用TensorFlow的其他计算函数来定义。</p>
<p><code>tf.placeholder</code>(dtype, shape=[])</p>
<p>其中<code>shape</code>为数据的形状，其第一维通常是样本数目，如果在运行之前不能确定样本数目，则第一维可以设为<code>None</code>。</p>
<p><code>tf.Variable</code>(initial_value, dtype, trainable=True)</p>
<p>创建一个Variable。模型的可训练权重必须声明为Variable类对象。</p>
<ul>
<li><strong><code>trainable</code></strong>: if set to True, then this variable will be automatically collected into the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code></li>
</ul>
<p><code>tf.variable_scope</code>(scope_name, reuse=bool)</p>
<p>开启一个变量的作用域。</p>
<ul>
<li><strong><code>scope_name</code></strong>:作用域名称</li>
<li><strong><code>reuse</code></strong>:变量是否可重用。如果<code>reuse=True</code>，则在该作用域下使用<code>get_variable(name,shape)</code>方法会先寻找是否有同名变量，如果有则返回同名变量，没有则新建变量；如果<code>reuse=False</code>，则在该作用域下使用<code>get_variable(name,shape)</code>方法会直接创建新变量。</li>
</ul>
<p>变量作用域的意义在于：当模型非常复杂时，可能会出现多个变量重名的情况，导致混淆；但如果强行修改变量名，使他们不相互重复，又会影响代码的可读性。</p>
<p>解决方法是：将模型中的变量分为不同的作用域，每个作用域内部的变量名不可重复，而不同作用域之间可以有同名的变量。</p>
<p>如何将变量加入作用域：在作用域下声明变量即可。</p>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;my_scope_name&#x27;</span>):</span><br><span class="line"><span class="meta">... </span>    w = tf.Variable(tf.zeros([<span class="number">1</span>]),name=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(w.name)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">my_scope_name/w:<span class="number">0</span></span><br></pre>

<p>以上语句将变量<code>w</code>加入了<code>my_scope_name</code>这个变量作用域中。如果输出它的名称，可以发现，将变量加入作用域的过程，实质上是在变量名前加上了作用域的名字。</p>
<p><code>tf.get_variable</code>(name, shape)</p>
<p>根据变量名<code>name</code>获取变量。如果当前作用域中不存在该名字的变量，或者当前作用域的<code>reuse=False</code>，那么该方法会生成一个新的变量。</p>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tf.get_variable(<span class="string">&#x27;w&#x27;</span>,shape=[<span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.name</span><br><span class="line"><span class="string">&#x27;w:0&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;my_scope_name&#x27;</span>):</span><br><span class="line"><span class="meta">... </span>    a = tf.get_variable(<span class="string">&#x27;w&#x27;</span>,shape=[<span class="number">1</span>])</span><br><span class="line"><span class="meta">... </span>    a.name</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="string">&#x27;my_scope_name/w:0&#x27;</span></span><br></pre>

<h2 id="在程序内创建数据"><a href="#在程序内创建数据" class="headerlink" title="在程序内创建数据"></a>在程序内创建数据</h2><p><code>tf.truncated_normal</code>(shape, stddev)</p>
<p><code>tf.zeros</code>(shape)</p>
<p><code>tf.constant</code>(value, shape)</p>
<h2 id="从外部加载数据"><a href="#从外部加载数据" class="headerlink" title="从外部加载数据"></a>从外部加载数据</h2><p><em><strong>Classes</strong></em></p>
<p><strong>Class</strong> <code>tf.train.Coordinator</code></p>
<p><strong><code>Description</code></strong></p>
<p>A coordinator for threads. This class implements a simple mechanism to coordinate the termination of a set of threads.</p>
<p><strong><code>Member functions</code></strong></p>
<p><code>tf.compat.v1.train.start_queue_runners</code>(sess=None, coord=None) -&gt; Starts all queue runners collected in the graph. </p>
<ul>
<li><strong><code>sess</code></strong>: <code>Session</code> used to run the queue ops. Defaults to the default session.</li>
<li><strong><code>coord</code></strong>: Optional <code>Coordinator</code> for coordinating the started threads.</li>
</ul>
<h2 id="构建计算图"><a href="#构建计算图" class="headerlink" title="构建计算图"></a>构建计算图</h2><p>计算图由数据节点和操作组成。计算图中的网络层操作（比如卷积、激活、池化等）大部分定义在<code>tf.nn</code>模块中。</p>
<p><code>tf.nn.conv2d</code>(input, filters, strides, padding, data_format, dilation, name)</p>
<ul>
<li><strong><code>filters</code></strong>: A <code>Tensor</code>. Must have the same type as <code>input</code>. A 4-D tensor of shape <code>[filter_height, filter_width, in_channels, out_channels]</code></li>
<li><strong><code>strides</code></strong>: An int or list of <code>ints</code> that has length <code>1</code>, <code>2</code> or <code>4</code>. The stride of the sliding window for each dimension of <code>input</code>. If a single value is given it is replicated in the <code>H</code> and <code>W</code> dimension. By default the <code>N</code> and <code>C</code> dimensions are set to 1. </li>
<li><strong><code>padding</code></strong>: Either the <code>string</code> <code>&quot;SAME&quot;</code> or <code>&quot;VALID&quot;</code> indicating the type of padding algorithm to use, or a list indicating the explicit paddings at the start and end of each dimension. When explicit padding is used and data_format is <code>&quot;NHWC&quot;</code>, this should be in the form <code>[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]</code>. When explicit padding used and data_format is <code>&quot;NCHW&quot;</code>, this should be in the form <code>[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]]</code>.</li>
<li><strong><code>dilations</code></strong>: An int or list of <code>ints</code> that has length <code>1</code>, <code>2</code> or <code>4</code>, defaults to 1. The dilation factor for each dimension of<code>input</code>. If a single value is given it is replicated in the <code>H</code> and <code>W</code> dimension. By default the <code>N</code> and <code>C</code> dimensions are set to 1. If set to k &gt; 1, there will be k-1 skipped cells between each filter element on that dimension. The dimension order is determined by the value of <code>data_format</code>, see above for details. Dilations in the batch and depth dimensions if a 4-d tensor must be 1.</li>
</ul>
<p><code>tf.nn.max_pool</code>(input, ksize, strides, padding, data_format=None, name=None)</p>
<ul>
<li><strong><code>input</code></strong>: Tensor of rank N+2, of shape <code>[batch_size] + input_spatial_shape + [num_channels]</code> if <code>data_format</code> does not start with “NC” (default), or <code>[batch_size, num_channels] + input_spatial_shape</code> if data_format starts with “NC”. Pooling happens over the spatial dimensions only.</li>
<li><strong><code>ksize</code></strong>: An int or list of <code>ints</code> that has length <code>1</code>, <code>N</code> or <code>N+2</code>. The size of the window for each dimension of the input tensor.</li>
<li><strong><code>strides</code></strong>: An int or list of <code>ints</code> that has length <code>1</code>, <code>N</code> or <code>N+2</code>. The stride of the sliding window for each dimension of the input tensor.</li>
<li><strong><code>padding</code></strong>: A string, either <code>&#39;VALID&#39;</code> or <code>&#39;SAME&#39;</code>. </li>
<li><strong><code>data_format</code></strong>: A string. Specifies the channel dimension. For N=1 it can be either “NWC” (default) or “NCW”, for N=2 it can be either “NHWC” (default) or “NCHW” and for N=3 either “NDHWC” (default) or “NCDHW”.</li>
<li><strong><code>name</code></strong>: Optional name for the operation.</li>
</ul>
<p><code>tf.nn.relu</code>(features, name=None)</p>
<p><code>tf.nn.softmax</code>(features, name=None)</p>
<p><code>tf.nn.softmax_cross_entropy_with_logits</code>(logits, labels)</p>
<ul>
<li><strong><code>logits</code></strong>:Tensor of size (B, n_classes). Model output without softmax activation.</li>
<li><strong><code>labels</code></strong>:Tensor of size (B, n_classes). Ground truth in the form of one-hot vectors.</li>
</ul>
<p><code>tf.argmax</code>(tensor[, axis])</p>
<p><code>tf.equal</code>(tensor1, tensor2)</p>
<p><code>tf.reshape</code>(tensor, shape)</p>
<p><code>tf.reduce_mean</code>(tensor)</p>
<h2 id="选择优化器"><a href="#选择优化器" class="headerlink" title="选择优化器"></a>选择优化器</h2><p>TensorFlow中，常用的优化器都继承自<code>tf.train.Optimizer</code>父类。</p>
<p><em><strong>Classes</strong></em></p>
<p><code>tf.train.RMSPropOptimizer</code>(learning_rate)</p>
<p><code>tf.train.AdamOptimizer</code>(learning_rate)</p>
<p><code>tf.train.Optimizer</code>有<code>minimize</code>方法，用来对目标函数进行反向传播。</p>
<p><strong><code>Member functions</code></strong></p>
<p><code>minimize</code>(loss, global_step=None, var_list=None, gate_gradients=GATE_OP, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)</p>
<ul>
<li><strong><code>loss</code></strong>: A <code>Tensor</code> containing the value to minimize.</li>
<li><strong><code>global_step</code></strong>: Optional <code>Variable</code> to increment by one after the variables have been updated.</li>
<li><strong><code>var_list</code></strong>: Optional list or tuple of <code>Variable</code> objects to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKeys.TRAINABLE_VARIABLES</code>.</li>
<li><strong><code>gate_gradients</code></strong>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li>
<li><strong><code>aggregation_method</code></strong>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li>
<li><strong><code>colocate_gradients_with_ops</code></strong>: If True, try colocating gradients with the corresponding op.</li>
<li><strong><code>name</code></strong>: Optional name for the returned operation.</li>
<li><strong><code>grad_loss</code></strong>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li>
</ul>
<pre class="highlight"><span class="line">tf_opt = tf.train.RMSPropOptimizer(LR)</span><br><span class="line">tf_step = tf_opt.minimize(tf_loss)</span><br></pre>

<p><em><strong>Functions</strong></em></p>
<p>如果需要随着训练的步数增加而减少学习率，可以使用<code>polynomial_decay</code>.</p>
<p><code>tf.train.polynomial_decay</code>(learning_rate, global_step, decay_steps, end_learning_rate=0.0001, power=1.0, cycle=False, name=None)</p>
<p>Applies a polynomial decay to the learning rate.</p>
<ul>
<li><strong><code>learning_rate</code></strong>: A scalar <code>float32</code> or <code>float64</code> <code>Tensor</code> or a Python number. The initial learning rate.</li>
<li><strong><code>global_step</code></strong>: A scalar <code>int32</code> or <code>int64</code> <code>Tensor</code> or a Python number. Global step to use for the decay computation. Must not be negative.</li>
<li><strong><code>decay_steps</code></strong>: A scalar <code>int32</code> or <code>int64</code> <code>Tensor</code> or a Python number. Must be positive. See the decay computation above.</li>
<li><strong><code>end_learning_rate</code></strong>: A scalar <code>float32</code> or <code>float64</code> <code>Tensor</code> or a Python number. The minimal end learning rate.</li>
<li><strong><code>power</code></strong>: A scalar <code>float32</code> or <code>float64</code> <code>Tensor</code> or a Python number. The power of the polynomial. Defaults to linear, 1.0.</li>
<li><strong><code>cycle</code></strong>: A boolean, whether or not it should cycle beyond decay_steps.</li>
<li><strong><code>name</code></strong>: String. Optional name of the operation. Defaults to ‘PolynomialDecay’.</li>
</ul>
<h2 id="建立会话并运行计算图"><a href="#建立会话并运行计算图" class="headerlink" title="建立会话并运行计算图"></a>建立会话并运行计算图</h2><p>TensorFlow的计算图的执行，必须在一个会话中进行。所以无论训练还是测试之前，都需要先开启一个会话。另外，在训练模式下，执行计算图之前，需要先对计算图中的Variable进行初始化。</p>
<p><em><strong>Classes</strong></em></p>
<p><strong>class</strong> <code>Session</code></p>
<p><strong><code>Description</code></strong></p>
<p>A class for running TensorFlow operations.</p>
<p>A <code>Session</code> object encapsulates the environment in which <code>Operation</code> objects are executed, and <code>Tensor</code> objects are evaluated. For example:</p>
<p><strong><code>Member functions</code></strong></p>
<p><code>__init__</code>(config)</p>
<ul>
<li><strong><code>config</code></strong>: An object of class <code>tf.Configproto</code></li>
</ul>
<p><code>run</code>(fetches, feed_dict=None)</p>
<p>The <code>fetches</code> argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types:</p>
<ul>
<li>A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Operation"><code>tf.Operation</code></a>. The corresponding fetched value will be <code>None</code>.</li>
<li>A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>. The corresponding fetched value will be a numpy ndarray containing the value of that tensor.</li>
<li>A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor"><code>tf.SparseTensor</code></a>. The corresponding fetched value will be a <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/SparseTensorValue"><code>tf.compat.v1.SparseTensorValue</code></a> containing the value of that sparse tensor.</li>
<li>A <code>get_tensor_handle</code> op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor.</li>
<li>A <code>string</code> which is the name of a tensor or operation in the graph.</li>
</ul>
<p><strong>Class</strong> <code>ConfigProto</code></p>
<p><strong><code>Member variables</code></strong></p>
<p><code>gpu_options</code>: An object of class <code>tf.GPUOptions</code></p>
<pre class="highlight"><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="literal">True</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</span><br><span class="line">session.run(init)</span><br></pre>

<p>训练时，我们的目的是根据目标函数来调整模型参数，因此最终需要运行的操作是<code>tf_step</code>.在运行<code>tf_step</code>这一操作时，需要给与它相关联的子图中的<code>placeholder</code>赋值。</p>
<pre class="highlight"><span class="line">fd = &#123;tf_data:batch_train_data, tf_labels:batch_train_labels&#125;</span><br><span class="line">session.run(tf_step, feed_dict=fd)</span><br></pre>

<p>测试时，我们的目的是计算能够表征模型效果的指标（比如在分类任务中，我们以模型分类的准确率作为指标），这时需要建立一个操作，它根据模型的输出和标签来计算指标的值。然后我们将测试数据赋值给<code>placeholder</code>，并运行该操作。</p>
<pre class="highlight"><span class="line">tf_acc = <span class="number">100</span>*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(tf_pred, <span class="number">1</span>), tf.argmax(tf_labels, <span class="number">1</span>))))</span><br><span class="line">fd = &#123;tf_data:test_data, tf_labels:test_labels&#125;</span><br><span class="line">accuracy = session.run(tf_acc, feed_dict=fd)</span><br></pre>

<h2 id="保存和恢复模型"><a href="#保存和恢复模型" class="headerlink" title="保存和恢复模型"></a>保存和恢复模型</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/keras/save_and_load">https://www.tensorflow.org/tutorials/keras/save_and_load</a></p>
<p><em><strong>Functions</strong></em></p>
<p><code>tf.get_checkpoint_state</code>(checkpoint_dir, <em>latest_filename</em>=None) -&gt; Union[None, GeneratedProtocolMessageType]</p>
<p>Returns CheckpointState proto from the “checkpoint” file.</p>
<p>If the “checkpoint” file contains a valid CheckpointState proto, returns it.</p>
<ul>
<li><strong><code>checkpoint_dir</code></strong>: The directory of checkpoints. </li>
<li><strong><code>latest_filename</code></strong>: Optional name of the checkpoint file.</li>
</ul>
<p><em><strong>Classes</strong></em></p>
<p><strong>class</strong> <code>tf.train.Saver</code></p>
<p><strong><code>Description</code></strong></p>
<p>Saves and restores variables.</p>
<p><strong><code>Member functions</code></strong></p>
<p><code>__init__</code>(max_to_keep=5, keep_checkpoint_every_n_hours)</p>
<ul>
<li><code>max_to_keep</code> indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, no checkpoints are deleted from the filesystem but only the last one is kept in the <code>checkpoint</code> file. </li>
<li><code>keep_checkpoint_every_n_hours</code>: In addition to keeping the most recent <code>max_to_keep</code> checkpoint files, you might want to keep one checkpoint file for every N hours of training. This can be useful if you want to later analyze how a model progressed during a long training session. For example, passing <code>keep_checkpoint_every_n_hours=2</code> ensures that you keep one checkpoint file for every 2 hours of training. The default value of 10,000 hours effectively disables the feature.</li>
</ul>
<p><code>save</code>(session, name, global_step=None)</p>
<ul>
<li><strong><code>sess</code></strong>: A Session to use to save the variables.</li>
<li><strong><code>save_path</code></strong>: String. Prefix of filenames created for the checkpoint.</li>
<li><strong><code>global_step</code></strong>: If provided the global step number is appended to <code>save_path</code> to create the checkpoint filenames. The optional argument can be a <code>Tensor</code>, a <code>Tensor</code> name or an integer.</li>
</ul>
<pre class="highlight"><span class="line">saver.save(sess, <span class="string">&#x27;my-model&#x27;</span>, global_step=<span class="number">0</span>) ==&gt; filename: <span class="string">&#x27;my-model-0&#x27;</span></span><br><span class="line">saver.save(sess, <span class="string">&#x27;my-model&#x27;</span>, global_step=<span class="number">1000</span>) ==&gt; filename: <span class="string">&#x27;my-model-1000&#x27;</span></span><br></pre>

<p><code>restore</code>(sess, load_path)</p>
<p><strong>class</strong> <code>Checkpoint</code></p>
<p><strong><code>Description</code></strong></p>
<p>Groups trackable objects, saving and restoring them.</p>
<h2 id="Tensor与其他数据类型相互转换"><a href="#Tensor与其他数据类型相互转换" class="headerlink" title="Tensor与其他数据类型相互转换"></a>Tensor与其他数据类型相互转换</h2><p><code>tf.convert_to_tensor</code>(data, dtype)</p>
<h2 id="Tensor元素类型转换"><a href="#Tensor元素类型转换" class="headerlink" title="Tensor元素类型转换"></a>Tensor元素类型转换</h2><p><code>tf.cast</code>(x, dtype) -&gt; return casted tensor</p>
<p><code>tf.to_float</code>(tensor)</p>
<h2 id="Tensor维度操作"><a href="#Tensor维度操作" class="headerlink" title="Tensor维度操作"></a>Tensor维度操作</h2><p><code>tf.stack</code>(list of tensors, axis)</p>
<p><code>tf.unstack</code>(tensor, axis)</p>
<p><code>x.shape</code>=<code>x.get_shape()</code>:求张量<code>x</code>的形状。返回值是<code>TensorShape</code>类型，可以用<code>as_list</code>方法将它转换为列表。</p>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.zeros([<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">TensorShape([Dimension(<span class="number">3</span>), Dimension(<span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.get_shape()</span><br><span class="line">TensorShape([Dimension(<span class="number">3</span>), Dimension(<span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.get_shape().as_list()</span><br><span class="line">[<span class="number">3</span>, <span class="number">2</span>]</span><br></pre>

<h2 id="Tensor数学计算"><a href="#Tensor数学计算" class="headerlink" title="Tensor数学计算"></a>Tensor数学计算</h2><p><code>tf.clip_by_value</code>(x, min, max)</p>
<h2 id="文件处理"><a href="#文件处理" class="headerlink" title="文件处理"></a>文件处理</h2><p><code>tf.read_file</code>(file_name) -&gt; return a file object</p>
<h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p><code>tf.image.decode_image</code>(image_file_object, channels) -&gt; return a tensor representing the image</p>
<p><code>tf.image.rgb_to_grayscale</code>(img_tensor)</p>
<p><code>tf.random_crop</code>(x, shape)</p>
<p><code>tf.image.resize_image</code>(images, shape, method), <code>method</code> can be one of:</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/image/ResizeMethod#BILINEAR"><code>ResizeMethod.BILINEAR</code></a></strong>: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bilinear_interpolation">Bilinear interpolation.</a></li>
<li><strong><a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/image/ResizeMethod#NEAREST_NEIGHBOR"><code>ResizeMethod.NEAREST_NEIGHBOR</code></a></strong>: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation">Nearest neighbor interpolation.</a></li>
<li><strong><a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/image/ResizeMethod#BICUBIC"><code>ResizeMethod.BICUBIC</code></a></strong>: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bicubic_interpolation">Bicubic interpolation.</a></li>
<li><strong><a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/image/ResizeMethod#AREA"><code>ResizeMethod.AREA</code></a></strong>: Area interpolation.</li>
</ul>
<h2 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h2><p><code>tf.stop_gradient</code>(input, name=None) -&gt; A <code>Tensor</code>. Has the same type as <code>input</code>. This operation will block the input from back-propogation during the running of computing graph.</p>
<p><code>tf.get_variable_scope</code>() -&gt; Returns the current variable scope.</p>
<p><code>tf.trainable_variables</code>() -&gt; Returns all variables created with <code>trainable=True</code>.</p>
<p><strong>Class</strong> <code>variable_scope</code>:</p>
<p>A context manager for defining ops that creates variables (layers).</p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope">https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope</a></p>
<p><strong><code>Member functions</code></strong></p>
<p><code>reuse_variables</code>(): set <code>reuse=True</code> in current scope.</p>
<h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p><code>tf.summary</code> module contains operations for writing summary data, for use in analysis and visualization.</p>
<p>The <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/summary"><code>tf.summary</code></a> module provides APIs for writing summary data. This data can be visualized in TensorBoard, the visualization toolkit that comes with TensorFlow. </p>
<p><em><strong>Functions</strong></em></p>
<p><code>tf.summary.image</code>(name, data) -&gt; Add an image summary to default graph. The name is added into <a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/GraphKeys#SUMMARIES"><code>GraphKeys.SUMMARIES</code></a> of default graph in current session. Returns True if success.</p>
<ul>
<li><strong><code>name</code></strong>: A name for this summary. The summary tag used for TensorBoard will be this name prefixed by any active name scopes.</li>
<li><strong><code>data</code></strong>: A <code>Tensor</code> representing pixel data with shape <code>[k, h, w, c]</code>, where <code>k</code> is the number of images, <code>h</code> and <code>w</code> are the height and width of the images, and <code>c</code> is the number of channels, which should be 1, 2, 3, or 4 (grayscale, grayscale with alpha, RGB, RGBA). Any of the dimensions may be statically unknown (i.e., <code>None</code>). Floating point data will be clipped to the range [0,1).</li>
</ul>
<p><code>tf.summary.scalar</code>(name, scalar)</p>
<p><code>tf.summary.merge_all</code>(key, scope) -&gt; Merges all summaries collected in the default graph.</p>
<ul>
<li><strong><code>key</code></strong>: <code>GraphKey</code> used to collect the summaries. Defaults to <a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/GraphKeys#SUMMARIES"><code>GraphKeys.SUMMARIES</code></a>.</li>
<li><strong><code>scope</code></strong>: Optional scope used to filter the summary ops, using <code>re.match</code></li>
</ul>
<p><em><strong>Classes</strong></em></p>
<p><strong>class</strong> <code>tf.summary.FileWriter</code>()</p>
<p><strong><code>Member functions</code></strong></p>
<p><code>__init__</code>(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None, filename_suffix=None, session=None)</p>
<ul>
<li><strong><code>logdir</code></strong>: A string. Directory where event file will be written.</li>
<li><strong><code>graph</code></strong>: A <code>Graph</code> object, such as <code>sess.graph</code>.</li>
<li><strong><code>max_queue</code></strong>: Integer. Size of the queue for pending events and summaries.</li>
<li><strong><code>flush_secs</code></strong>: Number. How often, in seconds, to flush the pending events and summaries to disk.</li>
<li><strong><code>graph_def</code></strong>: DEPRECATED: Use the <code>graph</code> argument instead.</li>
<li><strong><code>filename_suffix</code></strong>: A string. Every event file’s name is suffixed with <code>suffix</code>.</li>
<li><strong><code>session</code></strong>: A <a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Session"><code>tf.compat.v1.Session</code></a> object. See details above.</li>
</ul>
<p><code>add_summary</code>(summaries, global_step)</p>
<h2 id="运行设备配置"><a href="#运行设备配置" class="headerlink" title="运行设备配置"></a>运行设备配置</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/gpu">https://www.tensorflow.org/guide/gpu</a></p>
<p><em><strong>Classes</strong></em></p>
<p><strong>Class</strong> <code>GPUOptions</code></p>
<p><strong><code>Member functions</code></strong></p>
<p><code>__init__</code>(allow_growth=True)</p>
<h2 id="简洁开发"><a href="#简洁开发" class="headerlink" title="简洁开发"></a>简洁开发</h2><p>使用<code>tf.contrib.slim</code>模块可以更加方便地构建、训练和评估神经网络。<code>slim</code>中定义了各种常用的网络层，通过调用它们，可以将创建变量、初始化、计算输出的步骤用一个语句完成。</p>
<p><code>slim.conv2d</code>(input, out_c, [k_h, k_w], scope)</p>
<p><code>slim.fully_connected</code>(input, hidden_c, scope)</p>
<p><code>slim.max_pool2d</code>(input, [k_h, k_w], scope)</p>
<p><code>slim.repeat</code>(input, n_layers, layer_type, **args, scope):</p>
<p>将若干属于相同类型且具有相同参数的网络层堆叠在一起，前一层的输出是后一层的输入。</p>
<pre class="highlight"><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">&#x27;fc/fc_1&#x27;</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">&#x27;fc/fc_2&#x27;</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">&#x27;fc/fc_3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Equivalent, TF-Slim way using slim.stack:</span></span><br><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">&#x27;fc&#x27;</span>)</span><br></pre>

<p><code>slim.stack</code>(input, layer_type, **args, scope)</p>
<p>将若干属于相同类型但参数不同的网络层堆叠在一起，前一层的输出是后一层的输入。</p>
<pre class="highlight"><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">&#x27;core/core_1&#x27;</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">&#x27;core/core_2&#x27;</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">&#x27;core/core_3&#x27;</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">&#x27;core/core_4&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using stack:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">&#x27;core&#x27;</span>)</span><br></pre>

<p><code>slim.arg_scope</code>([layer_types], **arg_default_values)</p>
<p>开启一个参数作用域。在此作用域下声明的所有变量，只要其类型属于<code>[layer_types]</code>中的类型之一，那么它的参数会默认地用<code>**arg_default_values</code>进行初始化。在<code>arg_scope</code>中被指定的参数值，也可以在局部位置进行覆盖。</p>
<pre class="highlight"><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">&#x27;SAME&#x27;</span>,                weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>),scope=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">&#x27;VALID&#x27;</span>, weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>),scope=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">&#x27;SAME&#x27;</span>,</span><br><span class="line">weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>),scope=<span class="string">&#x27;conv3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Equivalent slim way using arg_scope:</span></span><br><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d], padding=<span class="string">&#x27;SAME&#x27;</span>,                  weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):                 </span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">&#x27;VALID&#x27;</span>, scope=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">&#x27;conv3&#x27;</span>)</span><br></pre>
  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/about">theme-kaze</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/2020/04/10/TensorFlow/">http://example.com/2020/04/10/TensorFlow/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2020/04/10/Shell脚本编程/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2020/04/10/Writing email/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow"><span class="toc-text">TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E7%A8%8B%E5%BA%8F%E5%86%85%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE"><span class="toc-text">在程序内创建数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E5%A4%96%E9%83%A8%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">从外部加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">构建计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">选择优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E4%BC%9A%E8%AF%9D%E5%B9%B6%E8%BF%90%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">建立会话并运行计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E6%81%A2%E5%A4%8D%E6%A8%A1%E5%9E%8B"><span class="toc-text">保存和恢复模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E4%B8%8E%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor与其他数据类型相互转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E5%85%83%E7%B4%A0%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor元素类型转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C"><span class="toc-text">Tensor维度操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">Tensor数学计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86"><span class="toc-text">文件处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><span class="toc-text">图像处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%93%8D%E4%BD%9C"><span class="toc-text">其他操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BD%95"><span class="toc-text">记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E8%AE%BE%E5%A4%87%E9%85%8D%E7%BD%AE"><span class="toc-text">运行设备配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%BC%80%E5%8F%91"><span class="toc-text">简洁开发</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/Kaze.png" class="author-img">

<p class="author-name">theme-kaze</p>
<p class="author-description">designed by theme-kaze</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>30</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>0</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>0</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow"><span class="toc-text">TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E7%A8%8B%E5%BA%8F%E5%86%85%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE"><span class="toc-text">在程序内创建数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E5%A4%96%E9%83%A8%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">从外部加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">构建计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">选择优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E4%BC%9A%E8%AF%9D%E5%B9%B6%E8%BF%90%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">建立会话并运行计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E6%81%A2%E5%A4%8D%E6%A8%A1%E5%9E%8B"><span class="toc-text">保存和恢复模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E4%B8%8E%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor与其他数据类型相互转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E5%85%83%E7%B4%A0%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor元素类型转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C"><span class="toc-text">Tensor维度操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">Tensor数学计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86"><span class="toc-text">文件处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><span class="toc-text">图像处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%93%8D%E4%BD%9C"><span class="toc-text">其他操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BD%95"><span class="toc-text">记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E8%AE%BE%E5%A4%87%E9%85%8D%E7%BD%AE"><span class="toc-text">运行设备配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%BC%80%E5%8F%91"><span class="toc-text">简洁开发</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
  </div>
</div>
  </article>
  
  <article class="card card-content">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow"><span class="toc-text">TensorFlow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E7%A8%8B%E5%BA%8F%E5%86%85%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE"><span class="toc-text">在程序内创建数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E5%A4%96%E9%83%A8%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">从外部加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">构建计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">选择优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E4%BC%9A%E8%AF%9D%E5%B9%B6%E8%BF%90%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">建立会话并运行计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E6%81%A2%E5%A4%8D%E6%A8%A1%E5%9E%8B"><span class="toc-text">保存和恢复模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E4%B8%8E%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor与其他数据类型相互转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E5%85%83%E7%B4%A0%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-text">Tensor元素类型转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E7%BB%B4%E5%BA%A6%E6%93%8D%E4%BD%9C"><span class="toc-text">Tensor维度操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97"><span class="toc-text">Tensor数学计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86"><span class="toc-text">文件处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><span class="toc-text">图像处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%93%8D%E4%BD%9C"><span class="toc-text">其他操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BD%95"><span class="toc-text">记录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E8%AE%BE%E5%A4%87%E9%85%8D%E7%BD%AE"><span class="toc-text">运行设备配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%BC%80%E5%8F%91"><span class="toc-text">简洁开发</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/Python 闭包的作用/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-09-12</div>
        <a href="/2020/09/12/C++ const关键字的用法/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-28</div>
        <a href="/2020/08/28/面试常见问题/"><div class="recent-posts-item-content"></div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2020-08-07</div>
        <a href="/2020/08/07/集成学习/"><div class="recent-posts-item-content"></div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2022
        </span>
        <a href="/" class="footer-link">theme-kaze demo </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('style', 'width: 100%; display: flex; justify-content: center;');
      img[i].parentElement.insertBefore(wrapper, img[i]);
      wrapper.appendChild(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>