<!DOCTYPE html>
<html>
<meta  lang="en" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <link rel="icon" href="/img/Kaze.png">
  <title>theme-kaze demo</title>
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js" as="script">
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_vpj3dq9ceqa.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/Kaze.png">
      
      <span class="navbar-logo-dsc">theme-kaze demo</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">Home </a>
    
    <a href="/archives" class="navbar-menu-item">Archive </a>
    
    <a href="/tags" class="navbar-menu-item">Tags </a>
    
    <a href="/categories" class="navbar-menu-item">Categories </a>
    
    <a href="/about" class="navbar-menu-item">About </a>
    
    <a href="/links" class="navbar-menu-item">Friends </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
  </div>
</nav>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      PyTorch使用手册
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2022-02-20T03:36:59.000Z" style="display: flex; align-items: center;">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2022-02-20</span>
    </time>
    
    
    <span class="dot"></span>
    <span>5.9k words</span>
    
  </div>
  
  <div class="post-meta post-show-meta" style="margin-top: -10px;">
    <div style="display: flex; align-items: center;">
      <i class="iconfont icon-biaoqian" style="margin-right: 2px; font-size: 1.15rem;"></i>
      
      
        <a href="/tags/Deep-learning-Programming/" class="post-meta-link">Deep learning, Programming</a>
      
    </div>
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h1 id="PyTorch使用手册"><a href="#PyTorch使用手册" class="headerlink" title="PyTorch使用手册"></a>PyTorch使用手册</h1><h2 id="1-在GPU上运行模型"><a href="#1-在GPU上运行模型" class="headerlink" title="1.在GPU上运行模型"></a>1.在GPU上运行模型</h2><p>PyTorch模型既可以在CPU上运行，也可以在GPU上运行。默认的运行设备是CPU，但为了加速模型训练和推断，经常要使用GPU。</p>
<p>要在GPU上运行模型，首先要判断当前环境中是否有可用的GPU：</p>
<pre class="highlight"><span class="line">torch.cuda.is_available()</span><br></pre>

<p>当环境中存在多块可用的GPU时，默认只使用第一块GPU（编号为0）。如果要使用其他的GPU，可以通过以下方法来修改环境变量CUDA_VISIBLE_DEVICES：</p>
<p>(1)在终端中设置：</p>
<pre class="highlight"><span class="line">CUDA_VISIBLE_DEVICES=1 python my_script.py# 使用1号GPU</span><br></pre>

<p>(2)在Python脚本中设置：</p>
<p><em><strong>注意，这种方式不能在Jupyter notebook中使用</strong></em></p>
<pre class="highlight"><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1,2&quot;</span></span><br></pre>

<p>(3)在IPython脚本或Jupyter notebook中设置：</p>
<pre class="highlight"><span class="line">%env CUDA_VISIBLE_DEVICES = <span class="string">&#x27;1,2&#x27;</span></span><br></pre>

<p>在GPU上运行模型，只需把模型和输入数据转换为GPU格式，其他部分和在CPU上运行无异。有以下两种方法将模型和输入数据转换为GPU格式：</p>
<p>（1）<code>cuda()</code></p>
<pre class="highlight"><span class="line">model = net().cuda()</span><br><span class="line">x = x.cuda()</span><br></pre>

<p>（2）<code>to</code> <em>(device)</em></p>
<pre class="highlight"><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">batch_input = batch[<span class="string">&#x27;img&#x27;</span>].to(device)</span><br><span class="line">output = model(batch_input)</span><br></pre>

<p>注： <code>Module.to</code>()是in-place方法，会将模型转换为GPU格式<code>(torch.cuda.FloatTensor)</code>；<code>Tensor.to</code>()不是in-place方法，它不会将<code>Tensor</code>转换为GPU格式，而是创造并返回一个GPU格式的副本。</p>
<p>在模型训练结束后，有时为了对输出数据（GPU格式）进行处理，需要将输出数据转换回CPU格式。这使用<code>Tensor.cpu</code>()即可。注意，在旧版本的Pytorch中，模型参数和数据都是<code>Variable</code>（封装了数据和梯度等信息），不能直接转换为numpy数组，需要先用<code>detach()</code>方法将数据从<code>Variable</code>中分离出来，再转换为numpy数组。</p>
<pre class="highlight"><span class="line">output = model(batch_input).cpu().detach().numpy()<span class="comment"># old</span></span><br><span class="line">output = model(batch_input).cpu().numpy()<span class="comment"># new</span></span><br></pre>

<h2 id="2-Variable"><a href="#2-Variable" class="headerlink" title="2.Variable"></a>2.Variable</h2><p>Variable是Tensor的封装，它除了包含有Tensor的全部数据之外，还包含有一些用于自动求导和反向传播的数据。要访问Variable对象所封装的Tensor的值，使用<code>Variable.data</code>。</p>
<h2 id="3-获取模块名"><a href="#3-获取模块名" class="headerlink" title="3.获取模块名"></a>3.获取模块名</h2><p>PyTorch中的模块都以python类的形式实现，而python中，一个类的名字可以通过<code>__name__</code>访问，而一个实例<em>instance</em>的所属类可以通过<code>type</code>*(instance)*来访问。</p>
<pre class="highlight"><span class="line">network = model()</span><br><span class="line">name = <span class="built_in">type</span>(network).__name__</span><br></pre>

<h2 id="4-数学函数"><a href="#4-数学函数" class="headerlink" title="4.数学函数"></a>4.数学函数</h2><blockquote>
<p><code>torch.max</code>(<em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em>)</p>
<p>求input在dim维度上的最大值。</p>
<p>返回：二元组，第一个元素：最大值，第二个元素：最大值索引</p>
</blockquote>
<pre class="highlight"><span class="line">&gt;&gt;&gt; a = torch.ones(3, 10, 10)</span><br><span class="line">&gt;&gt;&gt; b = torch.max(a, 0)</span><br><span class="line">&gt;&gt;&gt; b[0].shape</span><br><span class="line">torch.Size([10, 10])</span><br></pre>

<blockquote>
<p><code>torch.unique</code>(input): -&gt; 1-d tensor</p>
<p>求输入tensor的所有不重复元素。</p>
</blockquote>
<blockquote>
<p><code>torch.mean</code>(input)</p>
<p>求input的平均值</p>
</blockquote>
<blockquote>
<p><code>torch.norm</code>(input, p=2, dim=None)</p>
<p>求input的$L_p$向量范数</p>
<ul>
<li><strong><code>p</code></strong>: 范数，默认为2，可取其他数值，若$p=\infty$，则令<code>p=float(&#39;inf&#39;)</code></li>
<li><strong><code>dim</code></strong>: 维度，默认将所有元素展成1维向量然后求向量范数。<code>dim=i</code>表示，将所有其他维的下标都相同，只有第<code>i</code>维下标不同的元素组成一个行向量，对所有这样的行向量分别求向量范数。</li>
</ul>
</blockquote>
<h2 id="5-Tensor"><a href="#5-Tensor" class="headerlink" title="5.Tensor"></a>5.Tensor</h2><p><code>Tensor</code>是PyTorch中用于封装数据的类，模型的参数、输入、输出都需要转换为<code>Tensor</code>的形式。</p>
<hr>
<p><em><strong><code>创建Tensor</code></strong></em></p>
<p><strong>将已有数据转换为Tensor</strong></p>
<ul>
<li><code>torch.tensor</code>(<em>data</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em>, <em>pin_memory=False</em>)<ol>
<li><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</li>
<li><strong>dtype</strong> ([<code>torch.dtype</code>](https: //pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype), optional) – the desired data type of returned tensor. Default:  if <code>None</code>, <strong>infers data type from <code>data</code>.</strong></li>
<li><strong>device</strong> ([<code>torch.device</code>](https: //pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device), optional) – the desired device of returned tensor. Default:  if <code>None</code>, uses the current device for the default tensor type (see [<code>torch.set_default_tensor_type()</code>](https: //pytorch.org/docs/stable/torch.html#torch.set_default_tensor_type)). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li>
<li><strong>requires_grad</strong> ([<em>bool*](https: //docs.python.org/3/library/functions.html#bool)</em>,* <em>optional</em>) – If autograd should record operations on the returned tensor. Default:  <code>False</code>.</li>
<li><strong>pin_memory</strong> ([<em>bool*](https: //docs.python.org/3/library/functions.html#bool)</em>,* <em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default:  <code>False</code>.</li>
</ol>
</li>
</ul>
<p><strong>用指定常数初始化</strong></p>
<ul>
<li><code>torch.zeros</code>(*<em>size</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em>) -&gt; Tensor</li>
<li><code>torch.ones</code>(*<em>size</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em>) -&gt; Tensor</li>
<li><code>torch.nn.init.constant_</code><em>(tensor, val)</em> -&gt; <em>Tensor</em> :  Fill a Tensor using <em>val</em>.</li>
</ul>
<p><strong>随机初始化Tensor</strong></p>
<ul>
<li><code>torch.rand</code><em>(*size, out=None)</em></li>
<li><code>torch.randn</code><em>(*size, out=None)</em></li>
<li><code>torch.normal</code><em>(mean, std, out=None)</em></li>
<li><code>torch.linespace</code><em>(start, end, step, out=None)</em></li>
<li><code>torch.nn.init.normal_</code><em>(tensor, mean, std)</em> -&gt; <em>Tensor</em> :  Fill a Tensor using Gaussian with <em>mean</em> and <em>std</em>.</li>
</ul>
<hr>
<p><em><strong><code>class torch.Tensor</code></strong></em></p>
<p><strong><code>Methods</code></strong></p>
<ul>
<li><code>numpy</code><em>()</em></li>
<li><code>type</code><em>()</em> :  Return the data type of the tensor.</li>
<li><code>expand</code>(<em>size</em>) -&gt; Tensor:  Copy the tensor to <code>size</code>, return the new tensor.</li>
<li><code>expand_</code>(<em>size</em>) -&gt; None:  The inplace version of <code>expand</code>().</li>
</ul>
<pre class="highlight"><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; a = torch.ones((1, 3))</span><br><span class="line">&gt;&gt;&gt; b = a.expand((2, 3))</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br></pre>

<ul>
<li><code>squeeze</code><em>()</em>:  Delete all dimensions that have length==1, return the new tensor.</li>
<li><code>squeeze_</code><em>()</em>:  The inplace version of <code>squeeze</code>().</li>
<li><code>flatten</code>(<em>input</em>, <em>start_dim=0</em>, <em>end_dim=-1</em>) :  Flatten the tensor into 1-dim vector.</li>
<li><code>type</code>(<em>dtype=None</em>, <em>non_blocking=False</em>, **<em>kwargs</em>) -&gt; <em>str or Tensor</em> :  Returns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.</li>
<li><code>detach</code>() -&gt; <em>self</em> :  Detach a tensor from current graph, and returns itself.</li>
<li><code>cpu</code>() -&gt; <em>Tensor</em> :  Returns a copy of tensor in CPU memory. If the tensor is already on CPU, then no copy operation is performed.</li>
<li><code>cuda</code>(<em>device=None</em>, <em>non_blocking=False</em>) -&gt; <em>Tensor</em> :  Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</li>
</ul>
<hr>
<p><code>Tips</code>: </p>
<ol>
<li><p>通常来说，进行数学运算时，首先要使两个运算数的数据类型统一，对优先级较低的运算数作类型转换，使其与优先级较高的运算数类型相同。两个<code>Tensor</code>在进行数学运算时，也遵守这个规则。但当一个操作数是<code>Tensor</code>而另一个操作数不是<code>Tensor</code>时，则不会按照此惯例进行类型转换，而是会将另一个操作数转换成<code>Tensor</code>，且其数据类型与<code>Tensor</code>的数据类型相同。</p>
<p>另外，<code>Torch</code>重写了<code>&#39;/&#39;</code>运算符，它在Python3中表示浮点数运算，而在<code>Tensor</code>运算中，当两个<code>Tensor</code>都是整型时它表示整数除法。例如：</p>
</li>
</ol>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(<span class="number">5</span>) <span class="comment"># LongTensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor(<span class="number">2.0</span>) <span class="comment"># FloatTensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = <span class="number">2.0</span> <span class="comment"># float</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">type</span>(), b.<span class="built_in">type</span>()</span><br><span class="line">(<span class="string">&#x27;torch.LongTensor&#x27;</span>, <span class="string">&#x27;torch.FloatTensor&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a / b<span class="comment"># a -&gt; FloatTensor before div</span></span><br><span class="line">tensor(<span class="number">2.5000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a / c <span class="comment"># c -&gt; LongTensor before div. &#x27;/&#x27; means integer division in Torch.</span></span><br><span class="line">tensor(<span class="number">2</span>)</span><br></pre>

<ol start="2">
<li><p><code>torch.Tensor</code>*(data)*的参数是一个序列，创建的<code>Tensor</code>默认为浮点型(<code>FloatTensor</code>)。</p>
<p><code>torch.tensor</code><em>(data)<em>的参数可以是单个元素，创建的<code>Tensor</code>类型由</em>data</em>的类型决定。</p>
<p><code>torch.Tensor</code>([1])是<code>FloatTensor</code>，<code>torch.tensor</code>(1)则是<code>LongTensor</code>.</p>
</li>
<li><p>一个巨坑：在<code>torch=1.1.0</code>中没有<code>BoolTensor</code>类型，而在<code>torch=1.2.0</code>中加入了这一类型，并且一切<code>Tensor</code>逻辑运算的结果都属于<code>BoolTensor</code>类型。坑的地方在于：<code>sum</code><em>(x:  BoolTensor)<em>的结果还是<code>BoolTensor</code>，其值为0或1。因此，不像<code>torch=1.1.0</code>中可以通过<code>sum</code></em>(x)<em>统计<code>tensor</code>中真值的个数，在<code>torch=1.2.0</code>中必须先将</em>x</em>转换为其他数值类型才行。举例：</p>
</li>
</ol>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch.tensor <span class="keyword">as</span> tensor</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch=1.1.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">type</span>()</span><br><span class="line"><span class="string">&#x27;torch.ByteTensor&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sum</span>(a)</span><br><span class="line">tensor(<span class="number">2</span>, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch=1.2.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">type</span>()</span><br><span class="line"><span class="string">&#x27;torch.BoolTensor&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sum</span>(a)</span><br><span class="line">tensor(<span class="literal">True</span>)</span><br></pre>



<h2 id="6-Torchvision"><a href="#6-Torchvision" class="headerlink" title="6.Torchvision"></a>6.Torchvision</h2><p><code>Torchvision</code>是一个协助PyTorch、帮助用户更加方便地处理CV任务的第三方库。</p>
<p>它包含三个模块：</p>
<ul>
<li><code>transforms</code>：用于实现图片数据的转换、预处理和增强</li>
<li><code>datasets</code>：用于导入常用的CV数据集</li>
<li><code>models</code>：用于导入常用的CV模型及其预训练参数</li>
</ul>
<h3 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h3><p><code>transforms</code>模块中定义了多个类，它们都用于对图片对象（<code>PIL Image</code>, <code>Torch.tensor</code>）进行转换。这些类都提供了<code>__call__(image)</code>方法，使得它们的实例能够像函数一样被调用，调用的结果就是返回转换之后的图片对象。[参考官网](https: //pytorch.org/docs/stable/torchvision/transforms.html#functional-transforms)</p>
<p><em><strong><code>PIL Image</code>与<code>Tensor</code>之间转换</strong></em></p>
<ul>
<li><strong><code>ToTensor</code></strong>()：Convert a <code>PIL Image</code> or <code>numpy.ndarray</code> to tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] by dividing all elements by 256.0 if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8</li>
<li><strong><code>ToPILImage</code></strong>(<em>mode=None</em>)：Convert a tensor or an ndarray to PIL Image. Converts a torch. Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range.</li>
</ul>
<p><em><strong>对<code>PIL Image</code>进行转换</strong></em></p>
<p><em><strong>注意，以下方法返回的都是PIL Image。</strong></em></p>
<ul>
<li><strong><code>CenterCrop</code></strong>(<em>size</em>:  <em>int or tuple</em>)：将图片从中心位置裁剪成size指定的大小</li>
<li><strong><code>ColorJitter</code></strong>(<em>brightness=0</em>, <em>contrast=0</em>, <em>saturation=0</em>, <em>hue=0</em>)：对图片进行色彩增强（原理待学习）</li>
<li><strong><code>FiveCrop</code></strong>(<em>size:  int or tuple</em>)：将图片从四角及中心共5个位置裁剪成size指定的大小，注意它返回的是一个由image组成的tuple</li>
<li><strong><code>RandomCrop</code></strong>(<em>size</em>, <em>padding=None</em>, <em>pad_if_needed=False</em>, <em>fill=0</em>, <em>padding_mode=’constant’</em>)：将图片从任意位置裁剪成size指定的大小</li>
<li><strong><code>RandomHorizontalFlip</code></strong>(<em>p=0.5</em>)：以概率p对图像进行水平翻转</li>
<li><strong><code>RandomVerticalFlip</code></strong>(<em>p=0.5</em>)：以概率p对图像进行垂直翻转</li>
<li><strong><code>Resize</code></strong>(<em>size:  int or tuple, interpolation=2</em>)：将图像放缩到size指定的大小，<strong>如果size是一个数，那么会将图片按比例把短边缩放为size；</strong>如果size是一个元组(h, w)，那么会将图片缩放到该尺寸。interpolation表示所用的插值方法，默认为双线性插值</li>
<li><strong><code>Lambda</code></strong>(<em>function</em>)：返回一个用户自定义的transform。参数function为一个用户自定义的函数，它接受一个<code>PIL Image</code>作为参数，并返回处理后的<code>PIL Image</code>.</li>
</ul>
<p><em><strong>对<code>Torch.Tensor</code>进行转换</strong></em></p>
<ul>
<li><p><strong><code>Normalize</code></strong>(<em>mean=None, std=None</em>)：Normalize the image tensor. <strong>The length of <em>mean</em> must equal to the # of channels.</strong></p>
</li>
<li><p><strong><code>Compose</code></strong>(<em>transforms:  List</em>)：用于将几个transform组合在一起，形成一个transform</p>
</li>
</ul>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span>transforms.Compose([</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    transforms.CenterCrop(<span class="number">10</span>),</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>    transforms.ToTensor(),</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>])</span><br></pre>

<h3 id="datasets"><a href="#datasets" class="headerlink" title="datasets"></a>datasets</h3><h3 id="models"><a href="#models" class="headerlink" title="models"></a>models</h3><p><code>models</code>模块中实现了很多经典的CV模型（VGG，ResNet，DenseNet等等），并提供了它们的预训练参数。</p>
<h2 id="7-数据加载"><a href="#7-数据加载" class="headerlink" title="7.数据加载"></a>7.数据加载</h2><p>PyTorch的数据加载方法主要位于<code>torch.utils.data</code>模块中。这个模块下主要有两个类：<code>torch.utils.data.Dataset</code>和<code>torch.utils.data.DataLoader</code>。</p>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>这个类用于对数据进行封装，我们可以通过继承该类，来实现自己的数据集。它必须实现两个方法：</p>
<blockquote>
<p><code>__getitem__</code>*(self, index:  int)*：返回数据集中第index个样本</p>
<p><code>__len__</code>*(self)*：返回数据集中样本的总数</p>
</blockquote>
<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p><em><strong>注意，是DataLoader不是Dataloader。</strong></em></p>
<p>这个类用于在模型运行过程中，自动地向模型提供数据。它可以将一个数据集转换为可以索引的迭代器，其中每个元素都是一个batch的样本。</p>
<blockquote>
<p><code>torch.utils.data.DataLoader</code>(<em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>batch_sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=None</em>, <em>pin_memory=False</em>, <em>drop_last=False</em>, <em>timeout=0</em>, <em>worker_init_fn=None</em>, <em>multiprocessing_context=None</em>)</p>
</blockquote>
<h2 id="7-Module"><a href="#7-Module" class="headerlink" title="7.Module"></a>7.Module</h2><p>PyTorch中的网络层有两种实现方式：</p>
<ul>
<li>封装成类：它们都被实现为<code>torch.nn.Module</code>的子类</li>
<li>函数：可以直接调用，定义在<code>torch.nn.functional</code>模块中</li>
</ul>
<p>如果我们要自己实现模型，那么应该让它继承自<code>torch.nn.Module</code>类，此时它必须实现<code>forward</code>(<em>self, input</em>)方法。</p>
<hr>
<p><em><strong><code>class torch.nn.Module</code></strong></em></p>
<p><strong><code>Description</code></strong></p>
<p>Base class for all neural network modules. Your models should also subclass this class.</p>
<p><strong><code>Attributes </code></strong></p>
<p><strong><code>Methods</code></strong></p>
<ul>
<li><p><code>state_dict</code>(<em>destination=None</em>, <em>prefix=’’</em>, <em>keep_vars=False</em>)  -&gt;  dict</p>
<p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p>
</li>
<li><p><code>forward</code><em>(x)</em></p>
<p>Returns the forward result given input <code>x</code>.</p>
</li>
<li><p><code>parameters</code>(<em>recurse=True</em>)</p>
<p>Returns an iterator over all module params.</p>
</li>
<li><p><code>named_parameters</code>(<em>prefix=’’</em>, <em>recurse=True</em>) -&gt; iterator of <code>tuple(name, param)</code></p>
<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p>
</li>
<li><p><code>add_module</code>(<em>name</em>, <em>module</em>)</p>
<p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<ul>
<li><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be accessed from this module using the given name</li>
<li><strong>module</strong> ([*Module*](https: //pytorch.org/docs/stable/nn.html?highlight=parameters#torch.nn.Module)) – child module to be added to the module.</li>
</ul>
</li>
<li><p><code>apply</code>(<em>fn</em>)</p>
<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).</p>
</li>
<li><p><code>children</code>()</p>
<p>Returns an iterator over <strong><code>immediate</code></strong> children modules. Duplicate modules will be only returned once.</p>
</li>
<li><p><code>modules</code>()</p>
<p>Returns an iterator over <strong><code>all</code></strong> modules in the network.</p>
</li>
<li><p><code>zero_grad</code>()</p>
<p>Sets gradients of all model parameters to zero.</p>
</li>
</ul>
<hr>
<p><em><strong><code>class torch.nn.Parameter</code></strong></em></p>
<p><strong><code>Description</code></strong></p>
<p>A kind of Tensor that is to be considered a module parameter. Parameters are [<code>Tensor</code>](https: //pytorch.org/docs/stable/tensors.html#torch.Tensor) subclasses, that have a very special property when used with [<code>Module</code>](https: //pytorch.org/docs/stable/nn.html?highlight=parameters#torch.nn.Module) s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in [<code>parameters()</code>](https: //pytorch.org/docs/stable/nn.html?highlight=parameters#torch.nn.Module.parameters) iterator. </p>
<p><strong><code>Parameters</code></strong></p>
<ul>
<li><strong>data</strong> ([*Tensor*](https: //pytorch.org/docs/stable/tensors.html#torch.Tensor)) – parameter tensor.</li>
<li><strong>requires_grad</strong> ([<em>bool*](https: //docs.python.org/3/library/functions.html#bool)</em>,* <em>optional</em>) – if the parameter requires gradient. See [Excluding subgraphs from backward](https: //pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs) for more details. Default:  True</li>
</ul>
<hr>
<h3 id="预定义模块"><a href="#预定义模块" class="headerlink" title="预定义模块"></a>预定义模块</h3><ul>
<li><code>torch.nn.Conv2d</code>(<em>in_c, out_c, kernel_size, stride, padding</em>)</li>
<li><code>torch.nn.ConvTranspose2d</code>(<em>in_c, out_c, kernel_size, stride, padding, output_padding</em>)</li>
<li><code>torch.nn.Linear</code>(<em>in_n, out_n</em>)</li>
<li><code>torch.nn.Dropout</code>(<em>p</em>)<ul>
<li><strong><code>p</code></strong>:  The probability for a neuron to be deactivated.</li>
</ul>
</li>
<li><code>torch.nn.Sequential</code>(*<em>layers</em>)</li>
</ul>
<p><em><strong><code>ConvTranspose2d()</code>的执行过程</strong></em></p>
<p>对输入进行扩充-&gt;input padding-&gt;卷积-&gt;output padding，输出边长为：</p>
<p>$n’=n<em>s+(s-1)+p</em>2-k+1+op*2 = (n+1)*s+(p+op)*2-k$</p>
<p>例如：$n=64, k=4, s=2, p=1, op=0$, 则输出边长$n’=128$</p>
<h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><ul>
<li><p><code>torch.nn.LeakyReLU</code>(<em>negative_slope=0.01</em>, <em>inplace=False</em>)</p>
</li>
<li><p><code>torch.nn.LogSigmoid</code></p>
</li>
<li><p><code>torch.nn.ReLU</code>(<em>inplace=False</em>)</p>
<p><strong><code>Parameters</code></strong></p>
<ul>
<li><strong>inplace</strong>:  是否原地操作（改变输入），不影响计算结果</li>
</ul>
</li>
<li><p><code>torch.nn.Sigmoid</code></p>
</li>
<li><p><code>torch.nn.Tanh</code></p>
</li>
<li><p><code>torch.nn.Softmax</code>(<em>dim=None</em>)</p>
</li>
<li><p><code>torch.nn.LogSoftmax</code>(<em>dim=None</em>)</p>
</li>
</ul>
<h3 id="池化函数"><a href="#池化函数" class="headerlink" title="池化函数"></a>池化函数</h3><ul>
<li><p><code>torch.nn.AdaptiveAvgPool2d</code><em>(size)</em> :  对任意大小的输入特征图进行自适应池化，使其尺寸变为<em>size</em>所指定的尺寸，而特征通道数不变</p>
</li>
<li><p><code>torch.nn.AvgPool2d</code>(<em>kernel_size</em>, <em>stride=None</em>(default:  kernel_size), <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em>, <em>divisor_override=None</em>)</p>
</li>
<li><p><code>torch.nn.MaxPool2d</code>(<em>kernel_size</em>, <em>stride=None</em>(default:  kernel_size), <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em>)</p>
</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul>
<li><p><code>torch.nn.CrossEntropyLoss</code>(<em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction=’mean’</em>)</p>
<p>交叉熵，多分类模型的常用损失函数。其中内置了<code>LogSoftmax</code>操作，所以将它作为loss时，模型的最后就不需要添加softmax层了。</p>
<p>它可以用于多维交叉熵。比如语义分割之类的像素分类任务，其输入样本是2维的，对每个像素点都需要预测一个概率分布，此时模型的output形状为(B,n_classes,H,W)，而label为(B,H,W)；。又如句子词性预测，其输入样本是1维的，对每个单词都需要预测一个概率分布，此时模型的output形状为(B,n_classes,n_words)，而label为(B,n_words)。</p>
<p><strong><code>Parameters</code></strong></p>
<ul>
<li><strong>weight</strong>：<code>Tensor</code>.每一类的交叉熵的权重，默认为None，此时每个类的权重相等。在分类问题中，当各个类的样本数目不均衡时，可以通过设置每个类的<code>weight</code>，使其与该类的样本数成反比，来解决训练时过于侧重某一个类的问题。</li>
</ul>
<p><strong><code>__call__</code></strong>(<em>output,label</em>)</p>
<ul>
<li><strong>output</strong>:  Tensor of size (<em>B, n_classes, *dims</em>)</li>
<li><strong>label</strong>:  LongTensor of size (<em>B, *dims</em>). <code>*dims</code> should be the same as <code>output</code>.</li>
</ul>
</li>
<li><p><code>torch.nn.BCELoss</code>(<em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction=’mean’</em>)</p>
<p>二类交叉熵损失函数。其中没有内置<code>LogSoftmax</code>操作，所以将它作为loss时，模型的最后需要添加归一化激活层（sigmoid, softmax等）。</p>
<p>它可以直接应用于多维交叉熵（比如语义分割之类的像素分类任务），此时<code>output</code>形状为**<code>(B,1,*dims)</code>**，<code>label</code>类型为<code>FloatTensor</code>,形状为(B,*dims)</p>
<p>它也可以直接应用于多标签分类问题，此时<code>output</code>和<code>label</code>形状都为(B, n_classes, *dims)</p>
</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><blockquote>
<p>随机初始化</p>
</blockquote>
<p><code>torch.nn.init</code>模块中定义了网络层的初始化方法。</p>
<p>官方文档：https: //pytorch.org/docs/stable/nn.init.html?highlight=init</p>
<p>要初始化一个<code>nn.Module</code>类对象，首先要用<code>modules</code>()方法获取它的所有子模块（Pytorch中定义的基本网络层，比如线性层、卷积层等）：</p>
<pre class="highlight"><span class="line">net = model()</span><br><span class="line">net.modules()</span><br></pre>

<p>接下来要对每一个子模块进行初始化。有时对于不同种类的子模块要进行不同类型的初始化，这时可以用<code>isinstance()</code>方法来判断子模块的类别。对于Pytorch中定义的基本网络层，我们可以通过其成员变量<code>weight</code>访问其权重，<code>bias</code>访问其偏置（无偏置时，bias=None）</p>
<pre class="highlight"><span class="line">for m in model.modules(): </span><br><span class="line">    if isinstance(m, (nn.Conv2d, nn.Linear)): </span><br><span class="line">        nn.init.xavier_uniform(m.weight)</span><br></pre>

<p>常用的初始化方法有：</p>
<ul>
<li><p><code>torch.nn.init.normal_</code>(<em>tensor</em>, <em>mean=0.0</em>, <em>std=1.0</em>)</p>
</li>
<li><p><code>torch.nn.init.constant_</code>(<em>tensor</em>, <em>val</em>)</p>
</li>
<li><p><code>torch.nn.init.zeros_</code>(<em>tensor</em>)</p>
</li>
<li><p><code>torch.nn.init.xavier_uniform_</code>(<em>tensor</em>, <em>gain=1.0</em>)</p>
</li>
<li><p><code>torch.nn.init.xavier_normal_</code>(<em>tensor</em>, <em>gain=1.0</em>)</p>
</li>
<li><p><code>torch.nn.init.kaiming_uniform_</code>(<em>tensor</em>, <em>a=0</em>, <em>mode=’fan_in’</em>, <em>nonlinearity=’leaky_relu’</em>)</p>
</li>
<li><p><code>torch.nn.init.kaiming_normal_</code>(<em>tensor</em>, <em>a=0</em>, <em>mode=’fan_in’</em>, <em>nonlinearity=’leaky_relu’</em>)</p>
</li>
</ul>
<p><em><strong><code>torch.nn.functional</code></strong></em></p>
<ul>
<li><code>torch.nn.functional.conv2d</code>(<em>input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1</em>)<ul>
<li><strong>input</strong>:  tensor of size (B, C, H, W)</li>
<li><strong>weight</strong>:  weight of conv kernels, size (in_c, out_c, h, w)</li>
</ul>
</li>
</ul>
<pre class="highlight"><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># input image</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># conv kernels</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># conv result</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = F.conv2d(a, b, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line">tensor([[[[<span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>],</span><br><span class="line">          [<span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>],</span><br><span class="line">          [<span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>, <span class="number">3.5873e-43</span>]]]])</span><br></pre>

<ul>
<li><code>torch.nn.functional.binary_cross_entropy</code>(<em>input, label</em>)</li>
</ul>
<p>全连接层的权重通常使用高斯随机初始化，偏置通常初始化为0。</p>
<blockquote>
<p>用已有的Tensor进行初始化</p>
</blockquote>
<p>有时我们需要用已有的Tensor对模型参数进行初始化，比如使用预训练参数，或者自定义初始化值。</p>
<ul>
<li><p>加载预训练参数</p>
<p>如果我们训练的模型结构和预训练模型完全一致，那么可以直接用<code>model.load_state_dict(torch.load(***.pth))</code>方法来加载参数。</p>
<p>但有时我们训练的模型结构和预训练模型只有部分一致，而我们只想加载相同的部分的参数。这时可以将<code>load_state_dict</code>方法的<code>strict</code>参数设置为<code>False</code>，即只对键值相同的参数进行赋值。</p>
<h3 id="其他类"><a href="#其他类" class="headerlink" title="其他类"></a>其他类</h3><ul>
<li><code>torch.nn.flatten</code><em>()</em> :  将每个样本展平为1维向量</li>
</ul>
</li>
</ul>
<h2 id="8-优化器"><a href="#8-优化器" class="headerlink" title="8.优化器"></a>8.优化器</h2><p>[官方文档](https: //pytorch.org/docs/stable/optim.html) <code>torch.optim</code>模块实现了许多常用的模型优化器，包括SGD,Adam等等。所有优化器都继承自<code>torch.optim.Optimizer</code>类。</p>
<p><strong><code>基本用法</code></strong></p>
<ul>
<li>选择优化算法并创建一个优化器的实例，初始化时将它与待优化的参数绑定：</li>
</ul>
<pre class="highlight"><span class="line">net = model()</span><br><span class="line">optimizer = torch.optim.Optimizer(net.parameters())</span><br></pre>

<ul>
<li>在利用反向传播计算出各参数的梯度之后，使用<code>step()</code>方法来调整参数：</li>
</ul>
<pre class="highlight"><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = loss_function(output, label)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre>

<ul>
<li>每次更新之后，在下次更新之前，可能需要将上一次求出的梯度清零：</li>
</ul>
<pre class="highlight"><span class="line">optimizer.zero_grad()</span><br></pre>

<p><strong><code>API说明</code></strong></p>
<p><em><strong><code>class torch.optim.Optimizer</code></strong></em></p>
<ul>
<li><p><strong><code>Attributes</code></strong></p>
<ul>
<li><code>param_groups</code>:  a <code>list</code> of <code>dict</code> that preserves all parameter groups in this optimizer. Each <code>dict</code> corresponds to one parameter group, and it has several keys: <ul>
<li><strong>params</strong>:  <em>list of Tensors</em>. The parameters in that group.</li>
<li><strong>lr</strong>:  <em>float.</em> The learning rate of all parameters in this group.</li>
<li><strong>weight_decay</strong>:  <em>float.</em> The weight decay rate of this group.</li>
<li><strong>momentum</strong>:  <em>float</em>.</li>
<li><strong>dampening</strong>:  <em>float</em></li>
<li><strong>nesterov</strong>:  <em>boolean</em></li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>Methods</code></strong></p>
<ul>
<li><code>__init__</code><em>(params, defaults)</em><ul>
<li><strong>params</strong>:  an <code>iterable</code> of <code>torch.nn.Parameters</code> or <code>dict</code>.<ul>
<li><code>iterable</code> of <code>torch.nn.Parameters</code>:  all parameters in the iterator will be added to a single parameter group, sharing the same optimization options (such as lr, weight decay) defined by <code>defaults</code>.</li>
<li><code>iterable</code> of <code>dict</code>:  each <code>dict</code> corresponds to one parameter group, and has these keys:  <code>params</code>, and optimization options for this group like <code>lr</code> and <code>weight_decay</code>. By using this initiation method, you can use different optimization options on different parameters.</li>
</ul>
</li>
</ul>
</li>
<li><code>load_state_dict</code><em>(state_dict)</em>:  Loads the optimizer state.</li>
<li><code>state_dict</code><em>()</em>:  Returns the state of the optimizer as a [<code>dict</code>](https: //docs.python.org/3/library/stdtypes.html#dict).</li>
<li><code>step</code><em>()</em>:  Performs a single optimization step (parameter update).</li>
<li><code>zero_grad</code><em>()</em>:  Clears the gradients of all optimized [<code>torch.Tensor</code>](https: //pytorch.org/docs/stable/tensors.html#torch.Tensor) s.</li>
</ul>
</li>
</ul>
<p><em><strong><code>常用优化器</code></strong></em></p>
<ul>
<li><strong><code>torch.optim.Adam</code></strong>(<em>params</em>, <em>lr=0.001</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em>, <em>amsgrad=False</em>)</li>
<li><strong><code>torch.optim.Adagrad</code></strong>(<em>params</em>, <em>lr=0.01</em>, <em>lr_decay=0</em>, <em>weight_decay=0</em>, <em>initial_accumulator_value=0</em>, <em>eps=1e-10</em>)</li>
<li><strong><code>torch.optim.SGD</code></strong>(<em>params</em>, <em>lr</em>, <em>momentum=0</em>, <em>dampening=0</em>, <em>weight_decay=0</em>, <em>nesterov=False</em>)</li>
</ul>
<h2 id="9-保存-读取模型"><a href="#9-保存-读取模型" class="headerlink" title="9.保存/读取模型"></a>9.保存/读取模型</h2><blockquote>
<p><code>torch.save</code>*(state - dict, path)*：将模型状态（字典）保存为pth文件</p>
<p>模型的参数由<code>state_sict()</code>方法访问</p>
</blockquote>
<pre class="highlight"><span class="line">model = net()</span><br><span class="line">state = &#123;&#x27;net&#x27;: model.state_dict(), &#x27;epoch&#x27;: epoch&#125;</span><br><span class="line">torch.save(state, path)</span><br></pre>

<blockquote>
<p><code>torch.load</code>*(path, map_location)*：从pth文件中读取模型参数.</p>
<ul>
<li><strong><code>map_location</code></strong>: 指定将模型加载到哪个设备上运行。默认为GPU，如果要使用CPU，那么需要 <code>map_location=torch.device(&#39;cpu&#39;)</code></li>
</ul>
<p>模型利用<code>load_state_dict()</code>方法恢复参数</p>
</blockquote>
<pre class="highlight"><span class="line">model = net()</span><br><span class="line">state = torch.load(path)</span><br><span class="line">model.load_state_dict(state[&#x27;net&#x27;])</span><br><span class="line">epoch = state[&#x27;epoch&#x27;]+1</span><br></pre>



<h2 id="10-常见错误"><a href="#10-常见错误" class="headerlink" title="10.常见错误"></a>10.常见错误</h2><p><strong><code>1.SpatialClassNLLCriterion.cu: 103</code></strong></p>
<pre class="highlight"><span class="line">/tmp/pip-req-build-58y_cjjl/aten/src/THCUNN/SpatialClassNLLCriterion.cu: 103:  void cunn_SpatialClassNLLCriterion_updateOutput_kernel(T *, T *, T *, long *, T *, int, int, int, int, int, long) [with T = float, AccumT = float]:  block:  [0,0,0], thread:  [959,0,0] Assertion `t &gt;= 0 &amp;&amp; t &lt; n_classes` failed.</span><br></pre>

<p>错误分析：</p>
<p><code>SpatialClassNLLCriterion.cu: 103</code>：说明这是在计算NLLLoss的时候出错</p>
<p><code>Assertion t &gt;= 0 &amp;&amp; t &lt; n_classes failed.</code>：说明是label中的类数小于0（比如出现负值）或者超过了n_classes。</p>
<p>有两种可能：</p>
<p>1.label中出现负值或者有某些值超过了n_classes，这时应该检查一下加载数据时是否出现错误；</p>
<p>2.如果是在像素分类任务（比如语义分割、模糊分割等）中，需要计算2维的CrossEntropyLoss，那么注意：假设每个像素都可被分为n类，那么输出结果应该有n个通道，分别表示在n个类上的概率，即使是2分类，输出也应包含两个通道而不能只有一个（直接用BCELoss不就好了= =）。另外，label的形状应该是（B, H, W）的。</p>
<p><strong><code>2.模型输出过大或过小</code></strong></p>
<p>在训练一个多标签分类模型时，遇到了这样的问题：</p>
<pre class="highlight"><span class="line">RuntimeError:  reduce failed to synchronize:  device-side assert triggered</span><br></pre>

<p>同时附有以下输出：</p>
<pre class="highlight"><span class="line">/tmp/pip-req-build-58y_cjjl/aten/src/THCUNN/BCECriterion.cu: 42:  Acctype bce_functor&lt;Dtype, Acctype&gt;: : operator()(Tuple) [with Tuple = thrust: : detail: : tuple_of_iterator_references&lt;thrust: : device_reference&lt;float&gt;, thrust: : device_reference&lt;float&gt;, thrust: : null_type, thrust: : null_type, thrust: : null_type, thrust: : null_type, thrust: : null_type, thrust: : null_type, thrust: : null_type, thrust: : null_type&gt;, Dtype = float, Acctype = float]:  block:  [0,0,0], thread:  [64,0,0] Assertion `input &gt;= 0. &amp;&amp; input &lt;= 1.` failed.</span><br></pre>

<p>判断是求解损失函数<code>BCELoss()</code>时，出现output或label的值不在[0,1]之间的情况。</p>
<p>试着输出了前几个batch的模型输出和label，发现模型第1个epoch的输出还是正常的（介于[0,1]之间），之后就变成了非0即1，最后一个epoch甚至出现了NaN的输出，导致程序出现如前所述的报错。</p>
<p>由于模型最后一层的激活函数是<code>Sigmoid()</code>，因此这种情况意味着激活层的输入值要么异常大，要么异常小。猜测可能是由于学习率设置过大，导致在第一个batch的反向传播之后，模型的梯度变得过大，从而使得输出值爆炸。</p>
<p>将学习率由0.001调至0.0001，问题解决。</p>
<hr>
<p><strong><code>3.张量与常数</code></strong></p>
<p>在Pytorch中，Python常数被视为0维张量，而如果把它转换为Tensor，则它变成了1维张量，长度为1.</p>
<hr>
<p><strong><code>4.&#39;Modules can&#39;t have &#39;.&#39; in name&#39;</code></strong></p>
<p>PyTorch&gt;=0.3中，将不再允许模块名中带有’.’。</p>
<hr>
<p><strong><code>5.运行过程中显存占用越来越大</code></strong></p>
<p>问题代码：</p>
<pre class="highlight"><span class="line">average_loss = <span class="number">0.0</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> (batch_x, batch_y) <span class="keyword">in</span> data_loader: </span><br><span class="line">batch_x, batch_y = batch_x.cuda(), batch_y.cuda()</span><br><span class="line">pred = model(batch_x)</span><br><span class="line">loss = loss_function(batch_y, pred)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">average_loss += loss</span><br><span class="line">step += <span class="number">1</span></span><br><span class="line">average_loss /= step</span><br></pre>

<p>问题分析：</p>
<p>这段代码中将每个step的loss相加并取平均值。按理来说，每一次循环中都构建了一个计算图，而当本次循环结束时，该计算图会被释放掉。但是因为有语句<code>average_loss += loss</code>，这导致循环中构建的计算图成为了<code>average_loss</code>的计算子图中的一部分，而<code>average_loss</code>是循环之外的变量，所以每次循环之后，这个子图并不会被释放。随着循环的进行，计算图会越来越大，最终导致内存不足。</p>
<p>解决方案：</p>
<p>我们只想将每次循环后<code>loss</code>的值加到<code>average_loss</code>上。为了避免循环中构建的计算图成为<code>average_loss</code>计算子图的一部分，我们要在每次循环后将<code>loss</code>从计算图上分离下来，再加到<code>average_loss</code>上。</p>
<pre class="highlight"><span class="line">average_loss = <span class="number">0.0</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> (batch_x, batch_y) <span class="keyword">in</span> data_loader: </span><br><span class="line">batch_x, batch_y = batch_x.cuda(), batch_y.cuda()</span><br><span class="line">pred = model(batch_x)</span><br><span class="line">loss = loss_function(batch_y, pred)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">    <span class="comment"># detach loss from the computing graph</span></span><br><span class="line">average_loss += loss.detach().cpu()</span><br><span class="line">step += <span class="number">1</span></span><br><span class="line">average_loss /= step</span><br></pre>

<h2 id="11-CUDA"><a href="#11-CUDA" class="headerlink" title="11.CUDA"></a>11.CUDA</h2>
  </div>
  <div>
  
  <div class="post-note note-warning copyright" style="margin-top: 42px">
    <p><span style="font-weight: bold;">作者：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/about">theme-kaze</a></p>
    <p><span style="font-weight: bold;">文章链接：</span><a target="_blank" rel="nofollow noopener noreferrer" href="http://example.com/2022/02/20/pytorch_manual/">http://example.com/2022/02/20/pytorch_manual/</a></p>
    <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
  </div>
  
  </div>
</article>
<div class="nav">
  
  <div class="nav-item-prev">
    <a href="/2022/02/20/python_manual/" class="nav-link">
      <i class="iconfont icon-left nav-prev-icon"></i>
      <div>
        <div class="nav-label">Prev</div>
        
        <div class="nav-title">Python使用手册 </div>
        
      </div>
    </a>
  </div>
  
  
  <div class="nav-item-next">
    <a href="/2022/02/20/cnn/" class="nav-link">
      <div>
        <div class="nav-label">Next</div>
        
        <div class="nav-title">卷积神经网络 </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C"><span class="toc-text">PyTorch使用手册</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9C%A8GPU%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.在GPU上运行模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Variable"><span class="toc-text">2.Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%8E%B7%E5%8F%96%E6%A8%A1%E5%9D%97%E5%90%8D"><span class="toc-text">3.获取模块名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0"><span class="toc-text">4.数学函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Tensor"><span class="toc-text">5.Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Torchvision"><span class="toc-text">6.Torchvision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transforms"><span class="toc-text">transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#datasets"><span class="toc-text">datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#models"><span class="toc-text">models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-text">7.数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader"><span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Module"><span class="toc-text">7.Module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97"><span class="toc-text">预定义模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="toc-text">激活层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-text">池化函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%B1%BB"><span class="toc-text">其他类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">8.优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BF%9D%E5%AD%98-%E8%AF%BB%E5%8F%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">9.保存&#x2F;读取模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF"><span class="toc-text">10.常见错误</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-CUDA"><span class="toc-text">11.CUDA</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/Kaze.png" class="author-img">

<p class="author-name">theme-kaze</p>
<p class="author-description">designed by theme-kaze</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>23</span>
    <span>Posts</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>0</span>
    <span>Categories</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>6</span>
    <span>Tags</span>
  </a>
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C"><span class="toc-text">PyTorch使用手册</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9C%A8GPU%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.在GPU上运行模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Variable"><span class="toc-text">2.Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%8E%B7%E5%8F%96%E6%A8%A1%E5%9D%97%E5%90%8D"><span class="toc-text">3.获取模块名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0"><span class="toc-text">4.数学函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Tensor"><span class="toc-text">5.Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Torchvision"><span class="toc-text">6.Torchvision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transforms"><span class="toc-text">transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#datasets"><span class="toc-text">datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#models"><span class="toc-text">models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-text">7.数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader"><span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Module"><span class="toc-text">7.Module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97"><span class="toc-text">预定义模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="toc-text">激活层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-text">池化函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%B1%BB"><span class="toc-text">其他类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">8.优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BF%9D%E5%AD%98-%E8%AF%BB%E5%8F%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">9.保存&#x2F;读取模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF"><span class="toc-text">10.常见错误</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-CUDA"><span class="toc-text">11.CUDA</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>Categories</div>
  <div class="categories-list">
    
  </div>
</div>
  </article>
  
  <article class="card card-content">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>hot tags</div>
  <div class="tags-list">
    
    <a href="\tags\Programming" title="Programming"><div class="tags-list-item">Programming</div></a>
    
    <a href="\tags\Computer vision" title="Computer vision"><div class="tags-list-item">Computer vision</div></a>
    
    <a href="\tags\Machine learning" title="Machine learning"><div class="tags-list-item">Machine learning</div></a>
    
    <a href="\tags\机器学习" title="机器学习"><div class="tags-list-item">机器学习</div></a>
    
    <a href="\tags\Deep learning" title="Deep learning"><div class="tags-list-item">Deep learning</div></a>
    
    <a href="\tags\Deep learning, Programming" title="Deep learning, Programming"><div class="tags-list-item">Deep learning, Programming</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>TOC</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C"><span class="toc-text">PyTorch使用手册</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9C%A8GPU%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.在GPU上运行模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Variable"><span class="toc-text">2.Variable</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%8E%B7%E5%8F%96%E6%A8%A1%E5%9D%97%E5%90%8D"><span class="toc-text">3.获取模块名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0"><span class="toc-text">4.数学函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Tensor"><span class="toc-text">5.Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Torchvision"><span class="toc-text">6.Torchvision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transforms"><span class="toc-text">transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#datasets"><span class="toc-text">datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#models"><span class="toc-text">models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-text">7.数据加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader"><span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Module"><span class="toc-text">7.Module</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97"><span class="toc-text">预定义模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="toc-text">激活层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-text">池化函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%B1%BB"><span class="toc-text">其他类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">8.优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BF%9D%E5%AD%98-%E8%AF%BB%E5%8F%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">9.保存&#x2F;读取模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF"><span class="toc-text">10.常见错误</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-CUDA"><span class="toc-text">11.CUDA</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>Recent Posts</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-02-20</div>
        <a href="/2022/02/20/image_processing/"><div class="recent-posts-item-content">数字图像处理基础</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-02-20</div>
        <a href="/2022/02/20/python_manual/"><div class="recent-posts-item-content">Python使用手册</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-02-20</div>
        <a href="/2022/02/20/pytorch_manual/"><div class="recent-posts-item-content">PyTorch使用手册</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2022-02-20</div>
        <a href="/2022/02/20/cnn/"><div class="recent-posts-item-content">卷积神经网络</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2022
        </span>
        <a href="/" class="footer-link">theme-kaze demo </a>
      </div>
    </div>

    
    <div class="footer-dsc">
      
      
      Powered by
      <a href="https://hexo.io/" class="footer-link" target="_blank" rel="nofollow noopener noreferrer">&nbsp;Hexo </a>
      
      
      <span>&nbsp;|&nbsp;</span>
      
      
      
      Theme -
      <a href="https://github.com/theme-kaze" class="footer-link" target="_blank"
        rel="nofollow noopener noreferrer">&nbsp;Kaze</a>
      
    </div>
    
    
    
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton" >
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget">
  <i class="iconfont icon-weather button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('data-fslightbox', 'gallery');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('style', 'width: 100%; display: flex; justify-content: center;');
      img[i].parentElement.insertBefore(wrapper, img[i]);
      wrapper.appendChild(img[i]);
    }
    refreshFsLightbox();
  }
</script>
<script>loadScript("//cdn.jsdelivr.net/npm/fslightbox@3.1.0/index.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>